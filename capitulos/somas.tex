% !TEX encoding = UTF-8 Unicode

\chapter{Somas de variáveis independentes I: Lei dos Grandes Números}

Nesse capítulo introduziremos várias técnicas e resultados que serão úteis em geral, mas que aparecem naturalmente no estudo da somas de variáveis aleatórias independentes, que por sua vez é um assunto de extrema importância em teoria e aplicações de probabilidade.

\section{Convergência de variáveis aleatórias}

Antes de enunciar qualquer teorema sobre convergência de variáveis aleatórias, temos que definir o que isso significa.

\medskip

E facil de se dar conta que o conjunto $H$ de variáveis aleatórias em um dado espaço de probabilidade é um $\mathbb{R}$-espaço vectorial (ou um $\mathbb{C}$-espaço vetorial) pelas operações usuais de soma e  multiplicação por um escalar.
Mas como já reparamos que a maioria das propriedades que podemos provar só valem quase certamente, e mais razoável de trabalhar com o espaço quociente  $\cL^0=H/H_0$, onde $H_0$ e o conjunto das variáveis que valem $0$ quase certamente.

\medskip

Com um certo abuso de linguagem,
agora quando falamos de variável aleatória consideramos um elemento do espaço $\cL^0$.

\subsection{Espaços $\cL^p(P)$}

Dado $p\ge 1$ arbitrário, consideramos $\cL^p(P)$ o espaço quociente associado as variáveis tais que $|X|^p$ e integrável.

\medskip

Neste espaço reparamos que

$$\| X\|_p:= \left(E\left [ |X|^p \right]\right)^{1/p}.$$
define uma norma (a desigualdade triangular e simplesmente a desigualdade de Minkovski, e a positividade segue da nossa operação quociente).

\medskip

\begin{definition}
Seja $(X_n)_{n\ge 1}$ uma sequencia de variáveis aleatórias.
Falaremos que uma sequência de variáveis $X_n$ converge para $X$
 em $\cL^p$ se $X_n\in \cL^p$ para todos $n\ge 1$ e que a sequencia converge pela topologia associada a norma $\| \cdot \|_p$, o de jeito equivalente se
 \begin{equation}
 \lim_{n\to \infty}  E[ |X_n-X|^p]=0.
 \end{equation}

\end{definition}


\begin{remark}
Do mesmo jeito podemos definir convergência em $\cL^{\infty}$ o espaço de variável associado a norma
\begin{equation}
  \| X \|_\infty:= \min\{ t \ : \  P[X>t]=0 \},
\end{equation}
que é menos útil.
\end{remark}

\begin{exercise}
Se $X$ e uma variável positiva, mostra que $X\in \cL^p$ equivale a $\int x^{k-1}F_X(x) \dd x <\infty$, onde $X$ e a função acumulada de distribuição.
  Mostre uma fórmula análoga à da Proposição~\ref{p:espera_acumulada}.
\end{exercise}



\begin{exercise}
Seja $X$ uma variável aleatória e $q>p>1$ verificar que se $X\in \cL^q$ então
$X\in \cL^p$ e $\|X\|_p\le \|X\|_q$.
Concluir que convergência em $\cL^p$ implica convergência em $\cL^q$.
\end{exercise}

\begin{exercise}\label{ex:l1}
Seja $(X_n)_{n\ge 1}$ e uma sequência que converge em $\cL^p$ para $p\ge 1$.
Mostrar que $\lim_{n\to \infty} E[|X_n|^p]= E[|X|^p]$,
e que $\lim_{n\to \infty} E[X_n]= E[X]$.

\end{exercise}

Essas convergências são uteis em pratica nas computações mas não são as mais naturais a considerar do ponto de vista probabilístico.

\subsection{Convergência em probabilidade}

\begin{definition}
Seja $(X_n)_{n\ge 1}$ uma sequencia de variáveis aleatórias.
Falaremos que uma sequência de variáveis $X_n$ converge para $X$ em probabilidade se para qualquer $\gep>0$

\begin{equation}
\lim_{n\to \infty} P[ |X_n-X|>\gep ]=0.
\end{equation}
\end{definition}


\begin{exercise}
A convergência em probabilidade é a convergência induzida pela métrica
definida em $\cL^0$ por
\begin{equation}\label{l0metric}
 d(X,Y)=E[\max(|X-Y|,1)]
 \end{equation}
\end{exercise}


\begin{remark}
  Note que essa noção de convergência em $\cL^p$ para qualquer $p>1$ implica convergência em probabilidade.
  De fato, pela desigualdade de Markov temos
  \begin{equation}
    \lim_{n\to \infty} P[ |X_n-X|>\gep ]\le  \gep^{-p} P[ |X_n-X|^p]
  \end{equation}
\end{remark}

\subsection{Convergência quase certamente}

Falamos que $X_n$ converge quase certamente par $X$ se
existe $\bar \gO\subset \gO$ de probabilidade $1$ tal que para todos $\go\in \bar \gO$
\begin{equation}
\lim_{n\to \infty} X_n(\go)=X(\go).
\end{equation}
O limite e único em $\cL^0$.
Nota que convergência quase certamente de uma sequência implica convergência em probabilidade (com o mesmo limite).
Essa noção de convergência parece a mais natural mas vale a pena de reparar que ao contrario das outras, não corresponde a uma topologia.

\medskip

Isso e evidenciado pelo fato seguinte.

\begin{proposition}
Se uma sequência $X_n$ converge em probabilidade por $X$, então existe uma subsequencia que converge quase certamente.
\end{proposition}


\begin{proof}
Extraímos uma subsequência tal que
$$P(|X_{n(k)}-X|\ge k^{-1})\le k^{-2}.$$
Pelo Lemma de Borel-Cantelli aplicado ao evento
$A_k= [|X_{n(k)}-X|\ge k^{-1}]$
podemos concluir que $A_k$ não acontece infinitas vezes,
e então que com probabilidade $1$
$$\lim_{k\to \infty} X_{n(k)}(\go)=X(\go).$$
\end{proof}

\begin{remark}
  Observamos que uma consequência deste fato e que para usar teorema de convergência dominada, só precisaremos de ter uma sequência convergindo em probabilidade.
  \todo{Expandir essa observação}
\end{remark}



\begin{exercise}
Seja $(X_n)_{n\ge 1}$ e uma sequência de variáveis em $\cL^p$ $(p\ge 1)$ que converge em probabilidade para
$X\in \cL^p$.
Mostrar que $\lim_{n\to \infty} E[|X_n|^p]\le  E[|X|^p]$.
\end{exercise}

\begin{topics}
\section{Tópico: Integrabilidade uniforme}

Vimos que convergência em $\cL^1$ implica convergência em probabilidade.
O contrario e claramente falso, como se pode ver com a sequência de variáveis
$$X_n:= n\1_{\{\go \in [0,n^{-1}]\}},$$
onde $\gO=[0,1]$ e $P=U([0,1])$ que converge quase certamente para $0$ mas não converge em $\cL^1$ (Por exemplo $E[|X_n|=1]$)

\medskip

A razão da ausência de convergência aqui e uma concentração da esperança de $|X_n|$  (poderia ser também só uma parte dela)
em um evento cada vez menor do espaço de probabilidade ($[0,n^{-1}]$), que não aparece no limite. Em consequência
temos uma parte da norma $\cL^1$ que se perde no limite
(cf.\ Exercício \ref{ex:l1}) e temos $\lim_{n\to \infty} E[X_n]$).

\medskip

Uma sequência de variáveis em $\cL^1$ que não tem este tipo de comportamento se chama \textsl{uniformemente integral}.

\begin{definition}
 Uma coleção de variáveis aleatórias $(X_i)_{i\in \cI}$ e chamada uniformemente integrável se
 para qualquer $\gep>0$, existe $M>0$ tal que
 para todo $i\in \cI$
 \begin{equation}
  E[|X_{i}|\1_{[|X_i|>M]}]<\gep.
 \end{equation}
\end{definition}


\begin{example}
 \begin{itemize}
  \item [(a)] A coleção $\{X\}$ que contem só um elemento $X\in \cL^1$, é uniformemente integrável (e uma consequência trivial do teorema de convergência dominada).
  \item [(b)] Qualquer coleção finita de variáveis em $\cL^1$ é uniformemente integrável.
  \item [(c)] A bola unidade (o qualquer conjunto limitado) em $\cL^p$, $p>1$ e uniformemente integrável.
 \end{itemize}
\end{example}

Vamos agora ver uma caracterização alternativa da uniforme integrabilidade que justifica o nome é a seguinte.

\begin{proposition}
  Uma coleção de variáveis aleatórias $(X_i)_{i\in \cI}$  e uniformemente integrável se
  e só se para qualquer $\gep$ existe $\delta$ tal que para todos eventos  $A$ com $P(A)<\delta$ e todo $i \in \cI$
  \begin{equation}\label{eq:lacond}
    E[|X_i|\1_{A}]<\gep.
  \end{equation}
\end{proposition}

\begin{proof}
  Se a sequência for uniformemente integrável,
  podemos achar $M>0$ tal que para todo $n$
  \begin{equation}
    E[|X_i|\1_{[|X_i|> M]}]<\gep/2.
  \end{equation}
  Agora escolhemos $\delta=\gep/(2M)$.
  Se  $P(A)<\delta$ temos
  \begin{multline}
    E[|X_i|\1_{A}]= E[|X_i|\1_{A\cap [|X_i|\le  M]}]+E[|X_i|\1_{A\cap [|X_i|>  M]}]\\
    \le M P(A\cap [|X_i|\le  M])+ E[|X_i|\1_{[|X_i|>  M]}]\le M\delta +\gep/2\le \gep.
  \end{multline}
  Reciprocamente se \eqref{eq:lacond} for satisfeito, então em particular
  $$ C:=\sup_n E[|X_i|]<\infty,$$
  e por desigualdade de Markov temos
  \begin{equation*}
    P[|X_i|> M] \le \frac{C}{M}.
  \end{equation*}
  Então dado $\gep>0$, e $\delta$ tal que \eqref{eq:lacond} vale,
  podemos concluir escolhendo $M=C\delta^{-1}$ que
  $E[|X_i|\1_{[|X_i|> M]}]\le \gep$.
\end{proof}

Essa noção permite de deduzir convergência em $\cL^1$ de convergência em probabilidade.

\begin{theorem}
  Seja $X_n$ uma sequência de variáveis aleatórias que converge em probabilidade para $X$.
  As seguintes propriedades são equivalentes
  \begin{itemize}
  \item [(i)] $X_n$ converge em $\cL^1$ para $X$.
  \item [(ii)] $(X_n)_{n\ge 1}$ e uniformemente integrável.
  \end{itemize}
\end{theorem}

\begin{proof}
  Mostramos primeiro que $(i)$ implica $(ii)$.
  Primeiro a convergência implica que dado $\gep>0$ podemos achar $N$ tal que $n\ge N$ implica
  $$E[|X_n-X_N|]\le \gep/2.$$
  Agora, o conjunto $\{X_1,\dots,X_N\}$ sendo uniformemente integrável, existe $\delta$ tal que que
  se $P(A)<\delta$
  $$ \forall n\in \{1,\dots,N\}, \quad E[|X_n|\1_{A}]\le \gep/2.$$
  Para $n\ge N$ e $P(A)<\delta$, temos
  \begin{equation*}
    E[|X_n|\1_{A}]\le  E[|X_N|\1_{A}]+ E[|X_n-X_N|\1_{A}]\le \gep.
  \end{equation*}

  \medskip

  Agora provamos que $(ii)$ implica $(i)$.
  Notamos se $(X_n)_{n\ge 1}$ e uniformemente integrável então $X\in \cL_1$.
  Usando Fatou temos
  \begin{equation}
    E[|X|]\le \liminf_{n\to \infty}E[|X_n|]<\infty
  \end{equation}
  Agora reparamos que
  \begin{multline*}
    E[|X_n-X|]= E[|X_n-X| \ind_{|X_n-X|\le \gep} ]+ E[|X_n-X| \ind_{ [|X_n-X|>\gep] } ]\\
    \le
    E[|X_n| \ind_{ [|X_n-X|>\gep] } ]+E[|X|\ind_{ [|X_n-X|>\gep]}].
  \end{multline*}
  O primeiro termo e obviamente menor que $\gep$.
  Agora por consequência da convergência em probabilidade, a probabilidade de $[|X_n-X|>\gep]$ vai para zero.
  Por convergência dominada, isso implica que $E[|X|\ind_{ [|X_n-X|>\gep]}]\le \gep$ para $n$ grande suficiente.

  \medskip

  Agora usando integrabilidade uniforme, temos que se $P(A)<\delta$ para $\delta$ pequeno suficiente, $E[|X_n| \ind_{A} ]<\gep$, para qualquer $n$.
  Como $P [|X_n-X|>\gep]<\delta$ para $n$ grande suficiente $E[|X_n| \ind_{ [|X_n-X|>\gep] } ]<\gep$ o que permite concluir.
\end{proof}
\end{topics}



\begin{topics}
\section{Tópico: Completude de $\cL^q$ para $p\in \{0\}\in [1,\infty)$}


\subsection{Completude de $\cL^0$}

\begin{theorem}
Dado $(\gO,\cF,P)$ um espaço de probabilidade.
O espaço $\cL^0(P)$ equipado com a métrica  da convergência em probabilidade \eqref{l0metric} e completo.

\end{theorem}


\begin{proof}


Seja $(X_n)_{n\ge 0}$ uma sequência de Cauchy em $\cL^0$.
Podemos extrair ma subsequência $Y_k=X_{n(k)}$ tal que
\begin{equation}
\sup_{m>k} d(Y_k,Y_{k+1})\le  4^{-k}.
\end{equation}
Em particular temos usando a desigualdade de Markov, temos
\begin{equation}
P(|Y_k-Y_{k+1}|\ge 2^{-k})\le 2^{-k}.
\end{equation}
Pelo Lemma de Borel-Cantelli, obtemos que a sequência
$$Y_k=Y_1+\sum_{n=2}^k (Y_k-Y_{k-1}),$$
 converge q.c.\ Chamamos $X$ a variável obtida no limite (vista com elemento de $\cL^0$).

 \medskip

 Claramente $Y_k$ converge em probabilidade por $X$, mas usando a desigualdade triangular obtemos que
 $$d(X_n,X)\le d(Y_{n},X_n)+d(Y_n,X).$$
 A sequência $(X_n)_{n\ge 1}$ sendo de Cauchy o primeiro converge para zero e concluímos que
 $X_n$ converge para $X$ em probabilidade.
\end{proof}

\subsection{Completude de $\cL^p$, $p\ge 1$}


 \begin{theorem}
Dado $(\gO,\cF,P)$ um espaço de probabilidade.
O espaço normado $\cL^p(P)$ \eqref{l0metric} e completo para $p\ge 1$.

\end{theorem}

 \begin{proof}
 Seja $(X_n)_{n\ge 0}$ uma sequência de Cauchy em $\cL^q$.
  Como no caso anterior só precisamos de achar uma subsequência que converge para poder concluir.
 Como $(X_n)_{n\ge 0}$ e de Cauchy em $\cL^p$, podemos extrair uma sequência $(Y_k)_{k\ge 1}$ que converge quase certamente por uma variável $X$.
 Extraindo de nove se for necessário, podemos assumir que  (com a convençao $Y_0=0$).
   $$\sum_{k\ge 1} \| Y_k-Y_{k-1} \|_{p}<\infty.$$
    Por convergência monótona e desigualdade de triangular podemos concluir que
 $Z=\sum_{k\ge 1} |Y_k-Y_{k-1}|$ e um elemento de $\cL^p$.

 \medskip

 Agora podemos reparar que $Z^p$ domina a sequência $(Y^p_n)_{n\ge 1}$.
 Pois por convergência dominada podemos concluir que  $X=\sum_{k\ge 1} Y_k-Y_{k-1}$ pertencia a $\cL^p$ e que $E[|X-X_k|^p]$ converge para $0$.
 \end{proof}

 \begin{remark}
  Não faláramos nada a respeito do caso $\cL^{\infty}$  mas a demostração e muito mais fácil neste caso.
  Pode se verificar em particular que convergência em $\cL^{\infty}$ implica convergência quase certamente.
 \end{remark}


\subsection{Observações sob compacidade em espaços $\cL^p$}

\begin{proposition}\label{prop:laug}
 Seja $1 \le p<q$  $\cA\subset \cL^q(P)$, então se $\cA$ e pre-compacto (o fecho topológico e compacto) em $\cL^0(P)$ e
 limitado em $\cL^q(P)$, então e pre-compacto em $\cL^p(P)$.
 \end{proposition}
Uma consequência importante deste resultado e a seguinte
\begin{corollary}
 Se $(X_n)_{n\ge 1}$ converge em $\cL^0(P)$ e é limitada em $\cL^q(P)$, $q>1$ então
ela converge (pelo mesmo limite) em $\cL^p(P)$ para $p\in [1,q)$.
 \end{corollary}


 \begin{proof}
Por $\cA$ ser pre-compacto em $\cL^0$, toda sequência $(X_n)_{n\ge 1}$ em $A$ admite uma subsequência  $(Y_k)_{k\ge 1}$ convergente em $\cL^0$.
Para concluir temos que verificar que a convergência ocorre também em $\cL^p$. Por isso, e suficiente de mostrar que a sequência é de Cauchy.
Consideramos $\gep>0$ arbitrário.
Temos para qualquer $K>0$
\begin{equation}
E\left[ |Y_k-Y_l |^p \right] = E\left[ |Y_k-Y_l|^p \1_{\{|Y_k-Y_l |^p \le K\}} \right] +
E\left[ |Y_k-Y_l |^p \1_{\{|Y_k-Y_l |^p \ge K\}} \right].
\end{equation}
 Pelo segundo termo observamos que
 \begin{equation}
 E\left[ |Y_k-Y_l|^p \1_{\{|Y_k-Y_l|^p \ge K\}} \right]\le K^{p-q} E\left[ |Y_k-Y_l|^q \right]\le 2^q K^{p-q} \max_{n\ge 1} E[|X_n|^q]
 \end{equation}
 Se $K$ for grande suficiente, este termo e menos que $\gep/2$.

Por convergência dominada, se $k$ e $l$ forem grande suficiente (de um jeito que depende de $K$), temos
\begin{equation}
 E\left[ |Y_k-Y_l|^p \1_{\{|Y_k-Y_l|^p \le K\}} \right]\le \gep/2.
 \end{equation}

 \end{proof}


\end{topics}




\todosec{Tópico: Grafos Aleatórios}{fazer erdos renyi...}

\todosec{Tópico: Curie Weiss}{fazer...}


\section{Variância}

Digamos que estamos interessados em aproximar uma variável aleatória por uma constante de forma a minimizar o erro da aproximação.
Uma possível formulação desse problema é encontrar $a$ de forma a minimizar
\begin{equation}
  \label{e:EX_aproxima}
  E\Big( (X - a)^2 \Big) = E(X^2) - 2 a E(X) + a^2.
\end{equation}
Essa equação obviamente possui um único mínimo em $a = E(X)$.
Ao erro da aproximação acima damos o nome de variância.
Corresponde a distancia quadrada ao subespaço das constantes no espaço $\cL^2$.

\begin{definition}
  Dada uma variável aleatória $X \in \mathcal{L}^2$, definimos sua variância \index{variancia@variância} como
  \begin{equation}
    \Var(X) = E \Big( \big(X - E(X)\big)^2 \Big) = E(X^2) - E(X)^2.
  \end{equation}
\end{definition}


Podemos alternativamente entender a variância da seguinte maneira.
Sejam $X$ e $Y$ variáveis aleatórias independentes em $\mathcal{L}^2$ de mesma distribuição.
Então,
\begin{equation}
  \frac{1}{2} E\big( (X - Y)^2 \big) = \frac{1}{2}\left[  E(X^2) - 2 E(XY) + E(X^2)\right] = E(X^2) - E(X)^2 = \Var(X).
\end{equation}


\begin{proposition}[Propriedades básica da variança]
Observe pelas definições alternativas dadas acima que
\begin{enumerate}[\quad a)]
\item $\Var(X) \geq 0$.
\item Se $a, b\in \bbR$ $\Var (aX+b)=a^2 \Var(X).$
\item Mostre que se $X \in \cL^2$, então $\Var(X) = 0$ se e somente se $X = a$ quase certamente.
\end{enumerate}
\end{proposition}

\begin{proof}
 Exercício.
\end{proof}


\begin{exercise}
  Calcule $\Var(X)$ quando $X$ tem distribuições $\Ber(p)$, $U[0,1]$ ou $\Exp(\lambda)$.
\end{exercise}

A seguinte aplicação da propridade de Markov usa a variância para  estimar o quanto uma variável aleatória se desvia de sua média.
\begin{proposition}[Desigualdade de Chebychev]
  Se $X \in \mathcal{L}^2$ e $a > 0$, então
  \begin{equation}
    P [ |X - E(X)| > a] \leq \frac{\Var(X)}{a^2}.
  \end{equation}
\end{proposition}

\begin{proof}
  A desigualdade segue trivialmente da cota de Markov, ao observarmos que
  \begin{enumerate}[\quad a)]
  \item $|X - E(X)| \geq 0$.
  \item $|X - E(X)| > a$ se e somente se $|X - E(X)|^2 > a^2$.
  \item $E\big(|X - E(X)|^2\big) = E\big((X - E(X))^2\big) = \Var(X)$.
  \end{enumerate}
  Isso demostra a proposição.
\end{proof}

Mais geralmente, podemos usar a  desigualdade de Markov que para qualquer $X \in \cL^p$

\begin{equation}
  P[|X| \geq x] = P [X^p \geq x^p] \leq \frac{E(|X|^p)}{x^p}, \text{ para quaisquer $k \geq 1$.}
\end{equation}
Mas o caso $\cL^2$ é particularmente útil, como veremos posteriormente.
Isso por que  $\cL^2$ possui uma estrutura natural de espaço de Hilbert, e nesse contexto independência está relacionada a ortogonalidade.

\subsection{Variância, Soma e Independência }

Para variáveis aleatórias de média zero, a variância nada mais é que $E(X^2)$, ou em outras palavras $\lVert X \rVert^2_2$, o quadrado de sua norma em $\mathcal{L}^2$.
Isso nos motiva a olhar mais de perto para o produto interno em $\mathcal{L}^2$, que se traduz a $E(XY)$.
Mas para não nos restringirmos a variáveis de média zero, introduzimos a seguinte noção.

\begin{definition}
  Se $X, Y$ são variáveis em $\mathcal{L}^2$, definimos
  \begin{equation}
    \Cov(X,Y)
    = E\Big( \big(X - E(X)\big) \big(Y - E(Y)\big) \Big)
    = E(XY) - E(X)E(Y).
  \end{equation}
\end{definition}

Uma observação importante e a seguinte.
\begin{proposition}
  Se $X$ e $Y$ em $\mathcal{L}^2$ são independentes, então $\Cov(X,Y) = 0$.
\end{proposition}

\begin{proof}
 É uma consequência da Proposição \ref{prop:indep}.
\end{proof}

\begin{exercise}
  Sejam $X_1$ e $X_2$ as coordenadas canônicas em $\mathbb{R}^2$.
  Já vimos que elas não são independentes sob a distribuição $U_{S^1}$.
  Mostre que mesmo assim temos $\Cov(X_1, X_2) = 0$.
\end{exercise}

Considerando a covariança como um produto interno, podemos deduzir a seguinte formula para variança da soma.

\begin{proposition}
  Se $X_1, \dots, X_n$ são variáveis em $\mathcal{L}^2$, então
  \begin{equation}
    \Var(X_1 + \dots + X_n)
    = \sum_{i=1}^n \Var(X_i) + 2\sum_{i < j} \Cov(X_i, X_j).
  \end{equation}
  Em particular, se as variáveis $X_i$ forem independentes duas a duas, então
  \begin{equation}
    \label{e:var_linear}
    \Var(X_1 + \dots + X_n) = \sum_{i=1}^n \Var(X_i).
  \end{equation}
\end{proposition}

\begin{proof}
  Basta fazer o tedioso desenvolvimento
  \begin{equation}
    \begin{split}
      \Var\Big( \sum_{i=1}^n X_i\Big)
      & = E \Big( \Big( \sum_{i=1}^n X_i
        - E\Big( \sum_{i=1}^n X_i\Big)\Big)^2\Big)\\
      & = E \Big( \Big( \sum_{i=1}^n X_i - E(X_i)\Big)^2\Big)\\
      & = \sum_{i, j = 1}^n E \big(X_i - E(X_i)\big)
        E\big(X_j - E(X_j)\big),
    \end{split}
  \end{equation}
  o que termina a prova ao separarmos $i = j$ de $i \neq j$.
\end{proof}

\begin{exercise}
  Calcule $\Var(X)$ quando $X \overset{d}\sim \Bin(n, p)$.
\end{exercise}

\begin{exercise}
  Calcule $E(X)$ quando $X \overset{d}\sim \Geo(p)$.
\end{exercise}

Um dito popular muito comum no Brasil é que não devemos deixar todos os ``ovos no mesmo cesto'', o que nos remete à possibilidade de perdermos todos eles caso o cesto caia.
Uma outra maneira de pensar nas vantagens de se dividir nossos riscos entre várias fontes independentes de incerteza, vem da equação \eqref{e:var_linear}, melhor explicada no exercício abaixo.

\begin{exercise}
  Imagine que $X_1, \dots, X_n$ são variáveis \iid, tomando valores em $[0,1]$ e que temos um certo valor total $s \in \mathbb{R}_+$ que temos que apostar em $n$ caixas (dividindo como quisermos em $s_1, \dots, s_n$).
  Ao fim da semana, obteremos um retorno $S = \sum_{i=1}^n s_i X_i$.

  Calcule $E(S)$ e $\Var(S)$ nos dois casos seguinte:
  \begin{enumerate}[\quad a)]
  \item $s_1 = s$ e $s_i = 0$ para todo $i \geq 2$.
  \item $s_i = s/n$ para todo $i$.
  \end{enumerate}
  Compare os resultados.
\end{exercise}

\begin{exercise}
  Calcule $\lim_{p \to 0} F_p(x)$ onde $F_p$ é a função de distribuição acumulada de $p X_p$ com $X_p \overset{d}\sim \Geo(p)$.
  Você reconhece esse limite?
\end{exercise}

\section{Lei fraca dos grandes números}

Nessa seção iremos mostrar um dos resultados mais importantes da Teoria da Probabilidade.
O que nossa intuição tem a nos dizer sobre a probabilidade de obtermos um resultado em um dado é $1/6$?
Uma possível explicação seria por simetria, mas o que podemos dizer no caso de um dado viciado?

Se dizemos a alguém que a probabilidade de obter $6$ em um certo dado é $1/10$, naturalmente a pessoa pode se perguntar como descobrimos isso.
Um bom jeito de obter tal medida seria jogar o dado várias vezes independentemente e calcular em qual proporção dos ensaios ele retornou um seis.

O objetivo desta seção é confirmar a validade desse experimento de maneira quantitativa.

\begin{theorem}
  \index{Lei!Fraca dos Grandes Numeros@Fraca dos Grandes Números}
  \label{t:lei_fraca}
  Considerando $X_1, X_2, \dots$ são i.i.d.s em $\mathcal{L}^2(P)$, definimos
  \begin{equation}
    S_n = \sum_{i=1}^n X_i.
  \end{equation}
  Para todo $\varepsilon > 0$
  \begin{equation}
    \lim_{n \to \infty} P \Big[\Big| \frac{S_n}{n} - E(X_1)\Big| > \varepsilon \Big] = 0.
  \end{equation}
  Ou seja, $\tfrac{S_n}{n} \to E(X_1)$ em probabilidade.
\end{theorem}



\begin{proof}
  Sabemos que
  \begin{equation}
    P \Big[\Big| \frac{S_n}{n} - E(X_1)\Big| > \varepsilon \Big] \leq \frac{\Var(\tfrac{S_n}{n})}{\varepsilon^2},
  \end{equation}
  pois $E(S_n/n) = (1/n) E(X_1 + \dots + X_n) = E(X_1)$.

  Mas como $\Var(S_n/n) = (1/n^2) \Var (X_1 + \dots + X_n) = (n/n^2) \Var(X_1)$, temos o resultado.
\end{proof}

\begin{remark}
 Observe que nós apenas utilizamos que as variáveis $X_i$ eram independentes duas a duas.
 Isso e suficiente para mostrar que $S_n/n$ converge em $\cL^2$ e em consequência converge em probabilidade.
\end{remark}



Além disso, obtivemos o seguinte resultado quantitativo que vale mesmo para valores finitos de $n$:

\begin{scholia}
  Se $X_1, X_2, \dots$ são i.i.d.s em $\mathcal{L}^2$ e definimos $S_n = \sum_{i=1}^n X_i$ como acima, então, para todo $\varepsilon > 0$ e $n \geq 1$, temos
  \begin{equation}
    P \Big[\Big| \frac{S_n}{n} - E(X_1)\Big| > \varepsilon \Big] \leq \frac{\Var(X_1)}{\varepsilon^2 n}.
  \end{equation}
\end{scholia}





\begin{corollary}
  Se $A_1, A_2, \dots$ são eventos independentes dois a dois com $P(A_i) = p \in [0,1]$ para todo $i$, então
  \begin{equation}
    \lim_{n \to \infty} P \Big[ \Big| \frac{\#\{i \leq n; \omega \in A_i\}}{n} - p \Big| > \varepsilon \Big] = 0,
  \end{equation}
  ou em outras palavras a proporção de ensaios onde o evento $A_i$ ocorre converge em probabilidade para $p$.
\end{corollary}

\begin{proof}
  Basta tomar $X_i = \1_{A_i}$ no Teorema~\ref{t:lei_fraca}.
\end{proof}

\begin{exercise}
  Sejam $(X_i)_{i \geq 1}$ variáveis \iid com distribui\c{c}\~ao Ber$(p)$, $p \in [0,1]$. Mostre que
  \begin{equation}
    \lim_{N \to \infty} \frac 1N \sum_{i = 1}^N X_i X_{i+1} = p^2, \text{ em probabilidade.}
  \end{equation}
\end{exercise}

\begin{exercise}
  Sejam $X_1, \dots, X_n$ e $Y_1, \dots, Y_n$ variáveis independentes com distribuição $\Ber(p)$.
  Defina agora $Z_{i,j} = X_i Y_j$, para $i, j \in \{1, \dots, n\}$.
  \begin{enumerate}[\quad a)]
  \item Calcule a esperança de $S_n = \tfrac{1}{n^2} \sum_{i=1}^n \sum_{j=1}^n Z_{i,j}$.
  \item Estime $P[|S_n - E(S_n)| > a]$ usando o método do segundo momento. Como esse resultado se compara com o caso em que os $Z_{i,j}$ são i.i.d.?
  \end{enumerate}
\end{exercise}

\begin{exercise}
  \label{x:casas_tempestade}
  Considere uma rua infinita com casas $i \in \mathbb{Z}$.
  Para todo $i \in \mathbb{Z}$, existia uma rua entre as casas $i$ e $i+1$, mas após uma grande tempestade essas ruas foram danificadas.
  Mais precisamente, para cada $i \in \mathbb{Z}$, temos variáveis aleatórias $X_i$ que são i.i.d. com distribuição $\text{Ber}(p)$, onde $X_i = 1$ indica que o trecho da rua entre as casas $i$ e $i + 1$ foi danificado e não pode ser utilizado.
  Defina, para $i \in \mathbb{Z}$, $R_i$ como sendo o número de casas que continuaram acessíveis à casa $i$ após a tempestade.
  Por exemplo, se $X_{-2}$ e $X_0 = 1$ e $X_{-1} = 0$, temos que a casa $0$ somente pode acessar a casa $-1$, logo $R_0 = 1$.
  Nesse contexto,
  \begin{enumerate}[\quad a)]
  \item Calcule a distribuição e a esperança de $R_0$,
  \item Use o método do segundo momento para estimar a probabilidade
    \begin{equation}
      P \Big[ \Big| \frac{1}{n} \sum_{i=1}^n R_i - E(R_0) \Big| > a \Big].
    \end{equation}
  \end{enumerate}
\end{exercise}





\begin{topics}

\section{Tópico: Contando triângulos}

Vimos como a Lei Fraca dos Grandes Números seguiu de uma estimativa de segundo momento \index{momento!segundo} (mais precisamente usando a variância).

Nessa seção iremos mostrar como esse método é mais geral, se aplicando mesmo em situações onde as variáveis não são necessariamente independentes duas a duas.

Seja $V_n = \{1, \dots, n\}$ com $n \geq 3$ e $\mathcal{E}_n = \big\{ \{x,y\} \subseteq V_n; x \neq y \big\}$.
Chamamos o par $(V_n, \mathcal{E}_n)$ de grafo completo em $n$ vértices.

Definimos em um certo espaço de probabilidade $P_n$, as variáveis aleatórias $(X_e)_{e \in \mathcal{E}_n}$ de maneira \iid com distribuição $\Ber(p)$, onde $p \in [0,1]$.
Essas variáveis induzem um subgrafo aleatório $(V_n, \mathcal{E}_n')$, onde
\begin{equation}
  \mathcal{E}_n' = \big\{ e \in \mathcal{E}_n\ : \  X_e = 1 \big\}.
\end{equation}
Dizemos que os elos $e$, tais que $X_e = 1$ são abertos.

Definimos nesse espaço a variável aleatória
\begin{equation}
  T_n = \#\big\{\text{triângulos em $(V_n, \mathcal{E}_n')$}\big\}.
\end{equation}
Essa variável claramente pode ser escrita como
\begin{equation}
  T_n = \sum_{x,y,z \in V_n \text{ distintos}} \1_{A_{\{x,y,z\}}},
\end{equation}
onde $A_{\{x,y,z\}} = \big[\text{\{x,y,z\} formam um triângulo em $(V_n, \mathcal{E}_n')$}\big]$.

Gostaríamos de entender algo sobre a distribuição de $T_n$ e começamos calculando
\begin{equation}
  \begin{split}
    E^n(T_n) & = \sum_{\{x,y,z\} \text{ distintos}} P^n(A_{\{x,y,z\}})\\
    & = \binom{n}{3} p^3 = \frac{n(n-1)(n-2)}{6}p^3.
  \end{split}
\end{equation}
Logo, $P[T_n > a] \leq n(n-1)(n-2)p^3/6a$.
Mais ainda,
\begin{equation}
  \begin{split}
    E^n(T_n^2) & = \sum_{\{x,y,z\} \text{ distintos}} \quad \sum_{\{x',y',z'\} \text{ distintos}} P^n(A_{\{x,y,z\}} \cap A_{\{x',y',z'\}})\\
    & = \underbrace{\binom{n}{6} \binom{6}{3} p^6}_{\text{todos distintos}} + \underbrace{\binom{n}{5} \binom{5}{3} \binom{3}{1} p^6}_{\text{$1$-comum}} + \underbrace{\binom{n}{4} \binom{3}{2} \binom{4}{3} p^5}_{\text{$2$ em comum}} + \underbrace{\binom{n}{3}p^3}_{\text{iguais}}
  \end{split}
\end{equation}
Donde
\begin{equation}
  \Var^n(T_n) = \frac{1}{36} n^6 p^6 - \frac{1}{36} n^6 p^6 + c n^5 p^5 + ... \leq c (n^5 p^5 + n^3 p^3),
\end{equation}
para todos $p \in [0,1]$ e $n \geq 1$ se escolhemos bem a constante $c > 0$.

Isso nos permite por exemplo estimar o que acontece em alguns regimes, como por exemplo, se $p = 1/2$, então
\begin{equation}
  E^n(T_n) = \frac{n(n-1)(n-2)}{48},
\end{equation}
que cresce como $n^3$, e $\Var^n(T_n) \leq c n^5$, logo
\begin{equation}
  P^n\Big[ \Big|T_n - E^n(T_n)\Big| > \varepsilon n^3 \Big] \leq \frac{\Var^n(T_n)}{\varepsilon^2 n^6} \leq \frac{c}{\varepsilon^2 n}.
\end{equation}

\end{topics}

\todosec{Tópico: Análise de DNA}{fazer "computational molecular biology" - Pevzner seção 5.5...}

\todosec{Tópico: Método Probabilístico Revisitado}{usando segundo momento agora}





\section{Lei forte dos grandes números}

\begin{theorem}[Lei Forte dos Grandes Números]
  \index{Lei!Forte dos Grandes Numeros@Forte dos Grandes Números}
  \label{t:LFGN}
  Sejam $X_1, X_2, \dots$ \iid em $\mathcal{L}^1$, com $m = E(X_1)$.
  Então,
  \begin{equation}
    \lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n X_n = m, \text{ $P$-quase certamente.}
  \end{equation}
\end{theorem}

Começamos com uma prova do teorema no caso simples onde as variáveis estão em $\mathcal L^4$, que providencia um exemplo de uso do metodo dos momentos.

\begin{proof}[Demostração no caso $\mathcal L^4$]
Podemos supor que $E(X_i)=0$ (se não for o caso, considera $Y_i=X_i-E(X_i)$).
Podemos calcular o momento de ordem $4$ da soma expandido ela
Temos

\begin{equation*}
  E \left[ \left(\frac{1}{n} \sum_{i=1}^n X_n \right)^4 \right]= \frac{1}{n^4} \sum_{i_1,i_2,i_3,i_4=1}^n E[X_{i_1}X_{i_2}X_{i_3}X_{i_4}].
\end{equation*}
Pois usando independência, e $E(X_i)=0$ vemos que so os termos onde os $X_{k}$ apparecem pelo menos duas vezes não são nulos e
então que a soma acima vale
$$\frac{1}{n^4}\left( n E(X^4_1)-3n(n-1) E(X^2_1) \right)\le \frac{C}{n^2}.$$
Então temos
\begin{equation}
 \sum_{n=1}^{\infty} E \left[ \left(\frac{1}{n} \sum_{i=1}^n X_n \right)^4 \right]<\infty.
\end{equation}
Usando Fubini e equivalente a
\begin{equation}
 E \left[  \sum_{n=1}^{\infty} \left(\frac{1}{n} \sum_{i=1}^n X_n \right)^4 \right]<\infty.
\end{equation}
E em particular que quase certamente
\begin{equation}
 \sum_{n=1}^{\infty} \left(\frac{1}{n} \sum_{i=1}^n X_n \right)^4<\infty.
\end{equation}

\end{proof}


\begin{remark}
 Se as variáveis são positivas quase-certamente, então, usando o resultado para $\min(X_n,M)$ e o limite $M\to \infty$
 podemos concluir que $n^{-1}\sum_{i=1}^n X_i$ vai para infinito quase certamente.
 \end{remark}

 Se as variáveis não são integráveis, então podemos concluir que
 $n^{-1}\sum_{i=1}^n X_i$ não converge quase certamente.
 E uma consequência do resultado seguinte

 \begin{proposition}

 Se $(X_n)_{n\ge 1}$ e uma sequência de variáveis IID com $X_1\notin \cL_1$ então quase-certamente
 \begin{equation}
  \limsup_{n\to \infty} \frac{|X_n|}{n}=\infty.
 \end{equation}


 \end{proposition}

 \begin{proof}
 Temos para qualquer $M>0$
 $$E[|X_1|]=\int_0^{\infty} P[X_1\ge x] \dd x \le  M\left(1+ \sum_{n\ge 1} P[X_1\ge Mn]\right).$$
  Em particular se $X_1\notin \cL_1$ a soma a direita e infinita.

  \medskip

  Agora definimos o evento $A_n:= [X_n\ge nM]$.  Os $A_n$ são independente e
 $$\sum_{n\ge 1} P(A_n) = \sum_{n\ge 1} P[X_1\ge Mn]=\infty.$$
 Como temos
 $$ \left\{ \limsup_{n\to \infty} \frac{X_n}{n}\ge M \right\} \supset \left\{  \bigcap_{k\ge 1} \bigcup_{n\ge k} A_n \right\},$$
podemos concluir usando o Lema de Borel-Cantelli.
$$ P\left[ \limsup_{n\to \infty} \frac{X_n}{n}\ge M\right]\ge P \left[ \bigcap_{k\ge 1} \bigcup_{n\ge k} A_n  \right] =1.$$
Como $M$ e arbitrário podemos concluir.

 \end{proof}






\subsection{Demostração da Lei Fortes dos Grandes Números}

Vamos provar que para todo $a>E[X_1]$ temos quase certamente
\begin{equation}\label{lesup}
M:=\sup_{n\ge 0} S_n-n a <\infty
\end{equation}
onde $S_n:=\sum_{i=1}^n X_i$, $S_0=0$.

\medskip

A desigualdade \eqref{lesup} implica que
\begin{equation*}
 \limsup_{n\ge 0} \frac{S_n}{n}\le a,
\end{equation*}
e como $a$ pode ser arbitrariamente perto de $E[X_1]$,  $\limsup_{n\ge 0} \frac{S_n}{n}\le E[X_1]$.
A cota inferior pode ser obtido usando a desigualdade \eqref{lesup} para $-S_n$ (que também e uma soma de variáveis IID).


\medskip

Agora consideramos $a$ fixo.
Uma primeira coisa que podemos verificar e que coincide para qualquer $k\in \bbN$ temos
$$\{M<\infty\}=\{ \sup_{n\ge k} S_n-n a<\infty \}= \{\sup_{n\ge k} S_n-S_k-na \}$$
Em particular $\{M<\infty\}\in \sigma(X_{k+1},X_{k+2},\dots )$ para $k$ arbitrário, e por entanto pertencia na $\sigma$ álgebra caudal.
Em consequência da lei do $\{0,1\}$, temos $P[M<\infty]\in \{0,1\}$ e fica suficiente de provar $P[M<\infty]>0$ o de jeito equivalente
$P[M=\infty]<1$. Vamos prosseguir por contradição.

\medskip

Definimos
\begin{equation*}\begin{split}
                  M_k&:= \max_{1\le n \le k} (S_n-na)\\
                 M'_k&:= \max_{1\le n \le k} (S_{n+1}-S_n-na)
                 \end{split}
\end{equation*}
Essas duas sequências tem a mesma distribuição: Existe uma função $F$ tal que
$M_k=F(X_1,\dots,X_k)$ e $M'_k=F(X_2,\dots,X_{k+1})$ e os vetores $(X_1,\dots,X_k)$ e $(X_2,\dots,X_{k+1})$ tem a mesma distribuição.

\medskip

As sequências convergem de jeito crescente para $M$ e $M'$ respetivamente que são também identicamente distribuída
(observe que $P(M\le x)=\lim_{k\to \infty} P(M_k\le x)$).
Usando as definições temos
\begin{equation*}
 M_{k+1}=\max(0,M'_k+X_1-a)=M'_k-\min(M'_k,a-X_1).
\end{equation*}
Usando o fato que $M_k$ e $M'_k$ tem mesma distribuição obtemos

$$E[\min(M'_k,a-X_1)]=E[M'_{k}]-E[M_{k+1}]=E[M_{k}-M_{k+1}]\le 0.$$
Usando o teorema de convergência dominada, ($0\le \min(M'_k,a-X_1)\le |X_1-a|$) obtemos que
$$E[\min(M',a-X_1)]\le 0.$$
Agora para concluir, se tivermos $P(M'=\infty)=P(M=\infty)=1$, implicaria
$E[a-X_1]\le 0$,
o que e impossível com a nossa escolha de $a$.

\qed




\todosec{Tópico: Teorema de Weierstrass}{provar o teorema de Weierstrass de aproximação de funções contínuas por polinômios (prova probabilística). Ele é usado em convergência fraca em $\mathbb{R}$}

\todosec{Tópico: Entropia de Shannon}{fazer...}

\todosec{Tópico: Processos de renovação}{fazer...}


\begin{exercise}[Depende de \nameref{s:percolacao}]
  Considere o grafo $G = (\mathbb{Z}^2, E)$, onde $E = \big\{ \{x,y\}; |x - y|_1 = 1 \big\}$.
  Dotamos agora o espaço $\{0,1\}^E$ com a $\sigma$-álgebra $\mathcal{A}$ gerada pelas projeções canônicas $Y_e(\omega) = \omega(e)$, onde $\omega \in \{0,1\}^E$ e $e \in E$.
  Definimos o conjunto $A \subseteq \{0,1\}^E$ por
  \begin{equation}
    A = \Big[
    \begin{array}{c}
      \text{Existe uma sequência $(x_i)_{i\ge 0}$  de elementos distintos de $\mathbb{Z}^2$,}\\
      \text{tais que $e_i = \{x_i, x_{i+1}\} \in E$ e $Y_{e_i} = 1$ para cada $i \geq 0$}
    \end{array}
    \Big].
  \end{equation}
  \begin{enumerate}[\quad a)]
  \item Mostre que $A$ é mensurável com respeito a $\mathcal{A}$.
  \item Mostre que $A$ é um evento caudal, ou seja
    \begin{equation}
      A \in \bigcap_{\{K \subseteq E,\, K \text{ finito}\}} \sigma\big( Y_e; e \not \in K \big).
    \end{equation}
  \item Conclua que $P(A) \in \{0,1\}$.
  \end{enumerate}
\end{exercise}

\begin{exercise}
  Seja $\Omega = E^\mathbb{Z}$ um espaço produto infinito, dotado da $\sigma$-álgebra $\mathcal{A}$ gerada pelas projeções canônicas $(X_i)_{i \in \mathbb{Z}}$.
  Consideramos agora em $(\Omega, \mathcal{A})$ a medida produto $\mathbb{P} = P^{\otimes \mathbb{Z}}$, onde $P$ é uma probabilidade fixada no espaço polonês $(E, \mathcal{B}(E))$.
  \begin{enumerate}[\quad a)]
  \item Mostre que para qualquer evento $A \in \mathcal{A}$ e qualquer $\varepsilon > 0$, existe um $k \in \mathbb{Z}_+$ e um evento $A_k \in \sigma(X_i, |i| \leq k)$ tais que $\mathbb{P}[(A \setminus A_k) \cup (A_k \setminus A)] < \varepsilon$.
  \item Considere o shift $\theta:\Omega \to \Omega$ dado por $\theta(\omega)(i) = \omega(i-1)$ e mostre que se $A = \theta(A)$, então $P(A) \in \{0,1\}$.
  \end{enumerate}
\end{exercise}

\subsection{Lei dos grandes números em $\cL^1$}

Acabamos de provar convergência das somas de variáveis independentes no sentido quase certo.
Vamos ver agora que a convergência ocorre também em $\cL^1$.

\begin{theorem}
 Se $(X_n)_{n\ge 0}$ é uma sequência de variáveis independentes e de mesma distribuição com $X_1\in \cL^1$ então

 $$ \lim_{n\to \infty} \frac{1}{n}\sum_{k=1}^n X_k=E[X] $$
 em $\cL^1$.
\end{theorem}


\begin{proof}
Já sabemos que a convergência ocorre quase certamente e por consequência em probabilidade.
Para concluir, só precisamos mostrar que a sequência e uniformemente integrável usando Teorema \ref{t:ui}.
Usamos a notação $S_n=\sum_{k=1}^n X_k$.
Temos
\begin{equation}\label{eq:hop}
\frac{1}{n}E[|S_n| \ind_{[|S_n|\ge n M]}]\le \frac{1}{n}\sum_{i=1}^n E[X_i \ind_{[|S_n|\ge n M]}]\le E[X_1\ind_{[|S_n|\ge n M]}].
\end{equation}
 Usando a desigualdade de Markov temos que
 $$ P[|S_n|\ge nM]\le \frac{E[|S_n|]}{nM}\le E[X_1]/M.$$
Em particular  o termo a direita em \eqref{eq:hop} vai para zero quando $M\to \infty$ o que permite concluir.

 \end{proof}

\begin{exercise}
 Mostrar que se $X_1\in \cL^q$, $q>1$ então a convergência na lei dos grandes números vale também em $\cL^p$ para todos $p\in[1,q)$.
\end{exercise}

\begin{remark}
 Na verdade a lei dos grandes números vale em $\cL^p$ desde que $X_1\in \cL^p$ mas a demostração deste ultimo ponto e mais delicada.
\end{remark}

\newpage
\begin{topics}
\section[Tópico:Séries de Kolmogorov]{Tópico: Convergência de series aleatórias,\\
Teoremas de Uma, Duas e Três séries de Kolmogorov}


\subsection{Um criterio geral básico}

Sejà $X_1,\dots, X_n$ uma sequencia de variáveis aleatórias.
Queremos ter uma condição suficiente para a convergência de
\begin{equation}
S_n:= \sum_{i=1}^n X_i.
\end{equation}
Uma consequência do Teorema de Convergência Dominada e que
\begin{lemma}
 Se $(X_n)_{n\in \bbN}$ e uma sequencia de variáveis aleatórias integráveis tal que
 $\sum_{n=1}^{\infty} E[ |X_n| ]<\infty$,
então $\sum_{n=1}^{\infty} X_i$ e integrável.
Em particular a suma converge quase certamente.
\end{lemma}

O objetivo desse tópico e de presentar uns critérios mais eficaz no caso de soma de variáveis independente.

\medskip

\subsection{Caso de variáveis independentes: Os resultados}

Vamos começar com um exemplo para ilustrar o caso:\
Consideramos $(\gep_n)_{n\ge 1}$ uma sequencia de sinais aleatórios: são variáveis independente cuja distribuição e dada por
$$P[\gep_n=\pm 1]=1/2.$$
Queremos saber para quais valor de $p$ a soma $\sum_{n=1}^{\infty} \gep_n n^{-p}$ converge.

\medskip

E bastante fácil reparar (podemos por exemple usar o critério precedente) que temos convergência para $p>1$.

\medskip

O objetivo dos resultados que presentamos agora e de poder achar uma condição necessária e suficiente sobre o valor de $p$ para ter convergência.
E difícil ter uma intuição do resultado: sabemos que  $\sum_{n=1}^{\infty} n^{-p}$ diverge para todo $p\in (0,1)$ mas também que
$\sum_{n=1}^{\infty}(-1)^n n^{-p}$ converge (a seria satisfaz o teste da série alternada).

\medskip

Uma consequência dos resultados presentado abaixo e que no caso de sinais aleatórios, a convergência ocorre se $p>1/2$
(veremos mais tarde que esse resultado e o melhor possível).




\begin{theorem}[Uma serie]
 Sejà $(X_i)_{i\in \bbN}$ uma sequencia de variáveis independente tal que $E[X_i]=0$ para todo $i$,
 e que satisfaz também
 $$ \sum_{i=1}^\infty \Var X_i<\infty.$$
Temos então
\begin{equation}
P\left[\sum_{i=1}^{\infty} X_i \text{ converge } \right]=1.
\end{equation}
\end{theorem}

\medskip

\begin{theorem}[Duas Series]
 Sejà $(X_i)_{i\in \bbN}$ uma sequencia de variáveis independente
satisfaz
 $$ \sum_{i=1}^\infty \Var X_i<\infty.$$
Temos então
\begin{equation}
P[\sum_{i=1}^{\infty} X_i \text{ converge } ]=\begin{cases} 1 \quad \text{ se } \sum_{i=1}^\infty E[X_i] \text{ converge },\\
                                                0   \quad \text{ se } \sum_{i=1}^\infty E[X_i] \text{ não converge }.
                                              \end{cases}
\end{equation}
\end{theorem}

As vezes a variância pode não ser um bom criterio para avaliar convergência: $\Var(X_i)$ pode ser grande por causa de um evento que
acontece com pouca probabilidade. O teorema seguinte permite de tratar uns desses casos:
dado um real positivo $c>0$, definimos
\begin{equation}
X_i^c= X_i \1_{\{|X_i| \ge c\}}= \begin{cases} X_i \quad \text{ se } |X_i|\le c,\\
                                    0 \quad \text{ no caso contrário.}
                                   \end{cases}
           \end{equation}
Chamamos essas variáveis aleatórias as variâveis troncadas.

\begin{theorem}[Tres Series]
 Sejam $c>0$ um real positivo e $(X_i)_{i\in \bbN}$ uma sequencia de variáveis independente que
satisfaz
\begin{itemize}
 \item [(i)]  $\sum_{i=1}^\infty P[ |X_i|>c ]<\infty,$
 \item [(ii)]  $\sum_{i=1}^\infty E[ X^c_i]<\infty,$
 \item [(iii)]  $\sum_{i=1}^\infty \Var X^c_i<\infty.$
\end{itemize}
Então
$$ P[\sum_{i=1}^{\infty} X_i \text{ converge } ]= 1 $$
\end{theorem}



\begin{exercise}
Mostrar que para uma sequencia de variáveis independente $X=(X_i)_{i\in \bbN}$, com as notaçoes do teorema acíma,
$$ \exists c>0,\ \quad X \text{verifica $(i)-(ii)-(iii)$}  \quad \Leftrightarrow \quad \forall c>0,\ \quad X \text{verifica $(i)-(ii)-(iii)$}.$$

\end{exercise}


Os segundo a terceiro resultado são consequencia relativamente simples do primeiro.
Vamos dar uma prova deles logo
\begin{proof}[Prova to Teorema de Duas Series]
 Definimos $Z_i=X_i-E[X_i]$ e usando o Teorema de Uma Serie, concluimos que $\sum_{i=1}^{\infty} Z_i$ converge com probabilidade $1$.

 \medskip

 Pois usando propriedades basicas de limites, vemos que $$\sum_{i=1}^n X_i=\sum_{i=1}^n Z_i+ \sum_{i=1}^n E[X_i]$$ converge se e sò se
 a segunda sequência converge.
 \end{proof}

 \begin{proof}[Prova to Teorema de Tres Series]
Essa vez definimos $Y_i:= X_i \1_{\{|X_i| > c\}}$
Temos
\begin{equation}
 \sum_{i=1}^n X_i =  \sum_{i=1}^n X_i^c + \sum_{i=1}^n Y_i
\end{equation}
Usando o Teorema de Duas Series para $X_i^c$, jà podemos dizer que o primeiro termo converge com probabilidade $1$.
Pelo segundo termo, temos
\begin{equation}
 \sum_{i=1}^{\infty} P[Y_i\ne 0]=   \sum_{i=1}^{\infty} P[ X_i>c ]< \infty,
\end{equation}
e então pelo Lemma de Borel Cantelli, $$P[ Y_i \ne 0 \text{ infinitas vezes}]=0.$$
 o que permite de concluir.
\end{proof}


\begin{exercise}
Sejam $\alpha, \beta, \gamma$ tres numeros reais positívos e $(X_n)_{n\in \bbN}$ variáveis independentes com distribuição
 \begin{equation} \begin{split}
 P[X_n=1]&=\frac{1}{4}\left(2+n^{-\alpha}-n^{-\beta}\right), \\
 P[X_n=-1]&=\frac{1}{4}\left(2-n^{-\alpha}-n^{-\beta}\right) \\
 P[X_n= n]&= \frac{1}{2} n^{-\beta}.
 \end{split}
\end{equation}
Discute da convergência da serie seguinte em função do valor de $\alpha, \beta$ e $\gamma$
\begin{equation}
 \sum_{n=1}^{\infty} n^{-\gamma} X_n.
\end{equation}




\end{exercise}

\subsection{A desigualdade de Kolmogorov}

Para provar o Teorema, vamos precisar de uma feramenta util tambem em outro contexto.
E uma versão mais forte de desigualdade de Chebychev.

\medskip


\begin{theorem}[Desigualdade de Kolmogorov]
Suponhamos que $X_i$ são variáveis independente tal que $\Var X_i<\infty$ para todos $i$.
Então temos
\begin{equation}
 P[\max_{1\le k\le n} |S_k-E[S_k]|>\gep ]\le  \frac{\Var S_n}{\gep^2}.
\end{equation}

\end{theorem}


\begin{proof}
 Susbstituindo $X_i$ por $X_i-E[X_i]$ se for necessário, podemos supor que a esperança das variáveis vale zero.
 Vamos decompor o evento
 $$\left\{ \max_{1\le k\le n} |S_k|>\gep \right\}$$ com respeito ao primeiro valor de $k$ por qual $|S_k|>\gep$.
 \begin{equation}
  \begin{split}
E_1&:=\{|S_1|\ge \gep  \},\\
E_k&:=\{|S_1|<\gep;\, \dots ;\,  |S_{k-1}|<\gep\, ;  \, |S_k|\ge \gep  \}, \quad 2\le k\le n.
   \end{split}
\end{equation}
Obviamente temos $$\left\{\max_{1\le k\le n} |S_k|>\gep\right\}=\bigcup_{i=1}^n E_i.$$
A união sendo disjunta temos
\begin{equation}
P\left(\max_{1\le k\le n} |S_k|>\gep \right) \le  \sum_{i=1}^n P(E_i)\le  \sum_{i=1}^n E\left[  \frac{S^2_i}{\gep^2}\1_{E_i}\right].
\end{equation}
Vamos poder concluir a prova se mostramos para todo $k$
\begin{equation}\label{therest}
 E[S^2_k\1_{E_k}]\le E[S^2_n\1_{E_k}].
\end{equation}
Isso implica que
\begin{equation}
  \sum_{i=1}^n E\left[  \frac{S^2_i}{\gep^2}\1_{E_i}\right]\le E\left [  \frac{S^2_n}{\gep^2}  \sum_{i=1}^n \1_{E_i}\right]= \frac{\Var(S_n)}{\gep^2}.
\end{equation}
Para mostrar \eqref{therest}, definimos $\Delta_{k,n}= S_n-S_k$.
Temos
\begin{equation}
  E[S^2_n\1_{E_k}]= E[(S_k+\Delta_{k,n})^2\1_{E_k}]= E[S_k^2\1_{E_k}]+2 E[ \Delta_{k,n} S_k \1_{E_k}]+  E[ \Delta^2_{k,n}\1_{E_k}].
\end{equation}
O terceiro termo e obviamente positivo, então podemos concluir se mostramos  que $E[ \Delta_{k,n} S_k \1_{E_k}]=0$.
Lembramos que $S_k\1_{E_k}$ e uma funções das $k$ primeiras variáveis e $\Delta_{k,n}$ so depende das outras $n-k$
\begin{equation}
 S_k \1_{E_k}=f(X_1,\dots,X_k) \quad \text{ e } \quad \Delta_{k,n}=g(X_{k+1},\dots,X_{n}),
\end{equation}
Como os $X_i$ são independente, concluímos que  $S_k \1_{E_k}$ e $\Delta_{k,n}$ o são, e então

\begin{equation}
 E[ \Delta_{k,n} S_k \1_{E_k}]=E[\Delta_{n,k}S_k \1_{E_k}]=E[\Delta_{n,k}]E[S_k \1_{E_k}]=0.
\end{equation}
\end{proof}

\subsection{Prova to Teorema de Uma Serie}

Uma consequência imediata da desigualdade de Kolmogorov e que se \\ $\sum_{i=1}^{\infty} \Var X_i<\infty$ e $E[X_i]=0$
$$P\left[ \exists n\in \bbN,  |S_n| \ge A \right] \le \frac{\sum_{i=1}^{\infty} \Var X_i}{A^2}.$$
E obtida simplesmente pegando o limite quando $n$ vai para infinito da desigualdade.

\medskip

Vamos applicar essa formula pelo resto da soma. Definimos $n_k$ do jeito seguinte

\begin{equation}
 n_k:= \inf\left\{ n \sum_{i=n}^{\infty} \Var X_i \right\}<2^{-3k}.
\end{equation}
Aplicando a desigualdade de Kolmogorov temos
$$ P\left[ \exists r\ge n_k,  |\sum_{i=n_k}^r X_i| \ge 2^{-k} \right]\le 2^{-k}.$$
Então pelo Lema de Borel-Cantelli,
\begin{equation}
 P\left[ \exists k_0 \ \forall k\ge k_0, \forall r\ge n_k, \left|\sum_{i=n_k}^r X_i\right|\le 2^{-k} \right]=1.
\end{equation}
Agora nota que o evento  $$\{\exists k_0 \ \forall k\ge k_0, \forall r\ge n_k, |\sum_{i=n_k}^r X_i|\le 2^{-k}\}$$
implica  $$ \forall k\ge k_0,\, \forall m,p\ge n_k, \,
|S_m-S_p|\le 2^{-k+1}.$$
Então a sequencia e de Cauchy com probabilidade $1$ o que permite de concluir a prova. \qed




\subsection{Demostração da LGN usando o Teorema das Três Séries}

Antes de começar a prova, buscando inspiração no Teorema das Três Séries, mostraremos que basta considerar versões truncadas das variáveis $X_i$.
Isso é feito no próximo

\begin{lemma}
  \label{l:LFGN}
  Sejam $Y_i = X_i \1_{[|X_i| \leq i]}$.
  Então, para demonstrar o Teorema~\ref{t:LFGN}, basta provar que
  \begin{equation}
    \lim_{n \to \infty}\frac{1}{n} \sum_{i=1}^n Y_i = m, \text{ $P$-quase certamente.}
  \end{equation}
\end{lemma}

\begin{proof}[Prova do Lema~\ref{l:LFGN}]
  Consideramos os eventos $A_i = [X_i \neq Y_i]$.
  Obviamente,
  \begin{equation}
  \sum_{i=1}^{\infty} P(A_i) = \sum_{i=1}^{\infty} P[|X_i| \geq i] \leq
  \int_0^\infty P[|X_1| \geq t] \d t = E\big(|X_1|) < \infty.
  \end{equation}
  Logo, pelo Lema de Borel-Cantelli, temos que $P$-quase certamente $A_i$ acontece apenas finitas vezes.
  Digamos que $A_i$ não acontece para $i > N(\omega)$.
  Dessa forma, para qualquer $n \geq 1$,
  \begin{equation}
    \Big|\frac{1}{n}\sum_{i=1}^n (X_i - Y_i)\Big| \leq \frac{1}{n}\sum_{i=1}^n |X_i - Y_i| \leq \frac{1}{n} \sum_{i \leq N(\omega)} |X_i|,
  \end{equation}
  que converge para zero $P$-quase certamente, mostrando o resultado.
\end{proof}

O próximo passo para a prova da Lei Forte dos Grandes Números é cuidar da esperança das novas variáveis $Y_i$.
\begin{lemma}
  \label{l:lim_Z_n_LFGN}
  Sejam $Z_i = Y_i - E(Y_i)$, para $i \geq 1$ como acima.
  Então, para demostrar o Teorema~\ref{t:LFGN}, basta mostrar que
  \begin{equation}
    \label{e:lim_Z_n_LFGN}
    \lim_{n \to \infty}\frac{1}{n} \sum_{i=1}^n Z_i = 0, \text{ $P$-quase certamente.}
  \end{equation}
\end{lemma}

\begin{proof}
  Supondo a convergência em \eqref{e:lim_Z_n_LFGN}, sabemos que
  \begin{equation}
    \lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n Y_i - E(Y_i) = 0, \text{ $P$-quase certamente.}
  \end{equation}
  Mas $E(Y_i) = E(X_i \1_{[|X_i| \leq i]})$ que converge a $E(X_i) = m$, pelo Teorema da Convergência Dominada, donde concluímos que
  \begin{equation}
    \lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n E(Y_i) = m.
  \end{equation}
  Dessa forma, obtemos que $\tfrac 1n \sum_{i=1}^n Y_i$ converge quase certamente a $m$, donde concluímos a prova do Teorema~\ref{t:LFGN} por meio do Lema~\ref{l:LFGN}.
\end{proof}

Gostaríamos de utilizar os teoremas das séries para mostrar a convergência de $\tfrac 1n \sum_{n} Z_n$, mas obviamente, o fator $\tfrac 1n$ que precede a soma nos impede de fazê-lo.
O próximo resultado é um simples exercício de análise real, que nos permite reduzir a prova de \eqref{e:lim_Z_n_LFGN} para uma simples convergência de uma série sem pré-fatores.

\begin{lemma}[Lema de Kronecker]
  Suponha que $x_n \in \mathbb{R}$ e $b_n > 0$ sejam tais que $b_n \uparrow \infty$ e $\sum_{i=1}^\infty \frac{x_i}{b_i}$ convirja a $s \in \mathbb{R}$.
  Então
  \begin{equation}
    \lim_{n \to \infty} \frac{1}{b_n} \sum_{i=1}^n x_i = 0.
  \end{equation}
\end{lemma}

\begin{proof}
  Definindo $s_0 = 0$ e $s_n = \tfrac{x_1}{b_1} + \dots + \tfrac{x_n}{b_n}$, temos, por integração por partes,
  \begin{equation}
    \sum_{i=1}^n x_i = \sum_{i=1}^n b_i \frac{x_i}{b_i} = \sum_{i=1}^n b_i s_{i} - \sum_{i=1}^n b_i s_{i-1} = b_n s_n + \sum_{i=1}^{n-1} (b_{i} - b_{i+1}) s_{i}.
  \end{equation}
  Escolhemos agora, para qualquer $\varepsilon > 0$, um $n_0 \geq 1$ tal que $|s_n - s| < \varepsilon$ para todo $n \geq n_0$.
  Dessa forma,
  \begin{equation*}
    \begin{split}
      \frac{1}{b_n} \sum_{i=1}^n x_i & = s_n - \frac{1}{b_n}\sum_{i=1}^{n-1} (b_{i+1} - b_{i}) s_{i}\\
      & = s_n - \frac{1}{b_n}\underbrace{\sum_{i=1}^{n_0-1} (b_{i+1} - b_{i})}_{\Delta_{n_0}} s_{i} - \frac{1}{b_n}\sum_{i=n_0}^{n-1} (b_{i+1} - b_{i}) s_{i}\\
      & = \underbrace{s_n}_{\to s} - \underbrace{\frac{1}{b_n}\Delta_{n_0}}_{\to 0} - \underbrace{\frac{1}{b_n}\sum_{i=n_0}^{n-1} (b_{i+1} - b_i) s}_{= \tfrac{(b_n - b_{n_0})s}{b_n} \to s} - \underbrace{\frac{1}{b_n}\sum_{i=n_0}^{n-1} (b_{i+1} - b_{i}) (s_{i} - s)}_{\leq \varepsilon\tfrac{(b_n - b_{n_0})}{b_n} \leq \varepsilon},
    \end{split}
  \end{equation*}
  onde os limites indicados acima representam o que acontece quando $n \to \infty$.
  A prova segue do fato de $\varepsilon$ ter sido escolhido arbitrariamente.
\end{proof}

Estamos agora em posição de finalizar a
\begin{proof}[Prova do Teorema~\ref{t:LFGN}]
  De acordo com o Lema de Kronecker e o Lema~\ref{l:lim_Z_n_LFGN}, é suficiente mostrar que
  \begin{equation}
    \sum_{i=1}^n \frac{Z_i}{i}, \text{ converge quase certamente}.
  \end{equation}
  Por outro lado, como os $Z_i$'s tem média zero, o Teorema de Uma Série diz que é suficiente mostrar que
  \begin{equation}
    \sum_{i=1}^n \Var\Big(\frac{Z_i}{i}\Big) = \sum_{i=1}^n \frac{1}{i^2} \Var(Z_i) < \infty.
  \end{equation}
  Isso segue da seguinte estimativa
  \begin{equation}
    \begin{split}
      \sum_{i=1}^n \frac{1}{i^2} \Var(Z_i) & = \sum_{i=1}^n \frac{1}{i^2} \Var(Y_i) \leq \sum_{i=1}^n \frac{1}{i^2} E\big( X_i^2 \1_{[|X_i| \leq i]}\big)\\
      & = \sum_{i=1}^n \frac{1}{i^2} \sum_{k=1}^{i} E\big( X_1^2 \1_{[k-1 < |X_1| \leq k]}\big)\\
      & = \sum_{k=1}^n E\big( X_1^2 \1_{[k-1 < |X_1| \leq k]}\big) \sum_{i=k}^{n} \frac{1}{i^2}\\
      & \leq 2 \sum_{k=1}^n \frac{1}{k} E\big( X_1^2 \1_{[k-1 < |X_1| \leq k]}\big)\\
      & \leq 2 \sum_{k=1}^n E\big( X_1 \1_{[k-1 < |X_1| \leq k]}\big) \leq 2E(X_1) < \infty.
    \end{split}
  \end{equation}
  Isso nos permite concluir a prova de \eqref{e:lim_Z_n_LFGN} via o Lema de Kronecker.
  Consequentemente, obtemos o Teorema~\ref{t:LFGN} via o Lema~\ref{l:lim_Z_n_LFGN}.
\end{proof}

\begin{exercise}
  Sejam $Y_k$ variáveis aleatórias independentes e com a seguinte distribuição:
  \begin{equation}
    P[Y_k = i] =
    \begin{cases}
      \frac 12 - \frac 1{k^2} \quad & \text{se $i = 1$ or $i = -1$},\\
      \frac 2{k^2} & \text{se $i = 3$.}
    \end{cases}
  \end{equation}
  Mostre que
  \begin{equation}
    P\Big[ \frac 1n \sum_{k=1}^n Y_k \text{ converge a zero} \Big] = 1.
  \end{equation}
\end{exercise}

\begin{exercise}[Depende de \nameref{s:urna_polya}]
  Mostre que segundo a lei $P$ construida no Exercício~\ref{x:constr_Polya}, vale que
  \begin{equation}
    P\big[ \tfrac 1n \sum_{i-1}^n X_i \text{ converge}] = 1.
  \end{equation}
  Além disso calcule a distribuição do limite acima.
\end{exercise}
\end{topics}

\section{Momentos exponenciais}

Nessa seção desenvolveremos uma outra técnica para estimar a probabilidade de uma variável aleatória se desviar de sua esperança.

Já vimos o método do primeiro, segundo e quarto momento para controlar uma soma de variáveis independentes.
Um exemplo disso foi visto na estimativa
\begin{equation}
  P\Big[ \sum_{i=1}^n (X_i - E(X_i)) \geq a \Big] \leq \frac{\sum_i \Var (X_i)}{a^2}.
\end{equation}

Em geral, quanto maior o momento, melhor a estimativa do decaimento para a probabilidade de que uma variável se desvie de sua esperança.
Nessa seção iremos para momentos exponenciais, que em um certo sentido produzem estimativas ótimas para o comportamento assintótico da probabilidade de desvio.

Note que se quisermos uma pequena probabilidade de erro (como por exemplo $\sim 0.01$), o método do segundo momento é muito bom, como veremos posteriormente.
Mas se quisermos uma probabilidade de erro minúscula (em situações concretas, algo como $10^{-12}$ por exemplo), certamente teremos que aumentar bastante o valor de $n$, mas quanto?
As cotas de segundo momento são muito ruins para esse tipo de estimativa, nos levando a escolher um $n$ maior que o necessário.
Abaixo, desenvolveremos um método mais eficiente para responder a essa pergunta, obviamente sob certas hipóteses na distribuição das variáveis aleatórias.

\begin{definition}
  Dada uma variável aleatória $X$, definimos sua transformada de Laplace \index{trasformada!de Laplace} como
  \begin{equation}
    \phi_X(s) = E(\ex{s X}) \in (0, \infty],
  \end{equation}
  para todos $s \in \mathbb{R}$.
  Essa transformada também é chamada \emph{função geradora de momentos} de $X$. \index{funcao@função!geradora de momentos}
\end{definition}

\begin{exercise}
  Calcule a função geradora de momentos das distribuições $\Ber(p)$, $\Exp(\lambda)$ e $U_{[0,1]}$.
\end{exercise}

\begin{proposition}
  \label{p:propried_phi}
  Se $E(\ex{\delta |X|}) < \infty$, então
  \begin{enumerate}[\quad a)]
  \item $X \in \mathcal{L}^p$ para todo $1 \leq p < \infty$,
  \item $\phi_X(s) < \infty$ para todo $s \in (-\delta, \delta)$,
  \item $\phi_X(s)$ é $C^\infty$ em $(-\delta, \delta)$ e
  \item $\phi_X^{(n)}(s) = E(X^n \ex{sX})$.
  \item $s\mapsto \log \phi_{X_1}(s)$ e convexa no interval onde esta definida.
  \end{enumerate}
\end{proposition}

A última conclusão da proposição acima justifica a nomenclatura função geradora de momentos pois $\phi_X^{(n)}(0) = E(X^n)$.

\begin{proof}
  Obviamente, para todo $p \geq 1$ existe $c > 0$ tal que $\ex{\delta |x|} \geq c |x|^p$, donde $X \in \mathcal{L}^p$.
  Além disso, para todo $s \in (-\delta, \delta)$, temos $\phi_X(s) = E(\ex{s X}) \leq E(\ex{\delta |X|}) < \infty$, donde \textit{2.} segue imediatamente.

  Fixando $s \in \mathbb{R}$, vamos agora calcular
  \begin{equation}
      \frac{\phi_X(s + h) - \phi_X(s)}{h} = \frac{E\big(\ex{(s+h)X} - \ex{sX}\big)}{h} = E\Big(\ex{sX} \frac{\ex{hX} - 1}{h}\Big).
  \end{equation}
  Lembrando que $|\tfrac{1}{y}(e^y - 1)| \leq e^{|y|}$, para todo $y \in \mathbb{R}$, temos que para todos os $h < (\delta - |s|)/2$, o integrando acima é dominado por $|X| \ex{(|s| + h) |X|} \leq |X| \ex{\smash{\tfrac{\delta + |s|}{2} |X|}}$ que pertence a $\mathcal{L}^1$.
  Logo podemos usar o Teorema da Convergência Dominada para trocar o limite $h \to 0$ com a esperança, obtendo
  \begin{equation}
    \phi_X'(s) = E(X \ex{sX}).
  \end{equation}

  Note que para todo $\varepsilon > 0$ e $k \geq 1$, $|x|^k \leq c(k) \ex{\varepsilon |x|}$, isso nos permite repetir o argumento acima indutivamente para obter
  \textit{c)} e \textit{d)}.

  \medskip

  O ultimo ponto e simplesmente uma consequencia da desigualdade de H\"older. Para $\gl\in (0,1)$ temos
  \begin{multline}
   \phi_X( \gl a +(1-\gl)b)= E[e^{\gl a X} e^{(1-\gl)b X}] \\
   \le E[e^{a X}]^\gl E[ e^{b X}]^{(1-\gl)}=  (\phi_X(a))^{\gl}  (\phi_X(b))^{1-\gl}.
  \end{multline}

\end{proof}

Lembramos que ao usar o método do segundo momento, nos foi bastante útil o fato que a variância se comporta bem com relação a somas independentes.
Mais precisamente, $\Var(X_1 + \dots + X_k) = \Var(X_1) + \dots + \Var(X_k)$.

Uma outra propriedade importante da função geradora de momentos é que ela também se comporta bem com respeito à somas independentes.
\begin{proposition}
  Se $X_1, \dots, X_n$ são variáveis independentes com $\phi_{X_i}(s) < \infty$ para todo $i \leq k$ e $|s| < \delta$, então
  \begin{equation}
    \phi_{X_1 + \dots + X_k}(s) = \phi_{X_1}(s) \dotsm \phi_{X_k}(s), \text{ para todos $|s| < \delta$.}
  \end{equation}
\end{proposition}

\begin{proof}
  Basta observar que
  \begin{equation}
    \begin{split}
      E[\exp & \{s(X_1 + \dots + X_k)\}) = E[(\ex{sX_1} \dotsm \ex{sX_k})]\\
      & = E\big[\ex{sX_1}) \dotsm E(\ex{sX_k}\big] = \phi_{X_1}(s) \dotsm \phi_{X_k}(s),
    \end{split}
  \end{equation}
  usando Fubini.
\end{proof}

Consideraremos agora uma sequência $X_1, X_2, \dots$ de variáveis \iid com $\phi_{X_1}(s) < \infty$ para $|s| < \delta$.
Então podemos tentar estimar, para $a > 0$ e $|s| < \delta$,
\begin{equation*}
  \begin{split}
    P \Big[ & \frac{X_1 + \dots + X_n}{n} - E(X_1) \geq a \Big] = P \Big[ X_1 + \dots + X_n \geq (a + E(X_1)) n \Big]\\
    & \quad = P \Big[ \ex{s(X_1 + \dots + X_n)} \geq \ex{s (a + E(X_1)) n}\Big]\\
    & \quad \leq \phi_{X_1 + \dots + X_n}(s) \ex{-s (a + E(X_1))n } = \phi_{X_1}^n(s) \ex{-s (a + E(X_1))n }.
  \end{split}
\end{equation*}
O primeiro fator na estimativa acima pode crescer exponencialmente com $n$, enquanto o segundo decresce.
Gostaríamos que o comportamento do segundo predominasse, o que podemos concluir do seguinte argumento.

Sabemos que $\phi_{X_1}(s)$ é diferenciável em zero e que $\phi'_{X_1}(0) = E(X_1)$.
Logo, existe $s > 0$ tal que $\phi_{X_1}(s) < 1 + (E(X_1) + \tfrac{a}{2}) s$, donde
\begin{equation*}
  \begin{split}
    P \Big[ & \frac{X_1 + \dots + X_n}{n} - E(X_1) \geq a \Big] \leq \phi_{X_1}^n(s) \ex{-s (a + E(X_1))n }\\
    & \quad \leq \big(1 + (E(X_1) + \frac{a}{2})s \big)^n \ex{-s (E(X_1) + a)n }\\
    & \quad \leq \exp\Big\{ s \Big( E(X_1 + \frac{a}{2} - E(X_1) - a) n \Big) \Big\} = \ex{-san/2}.
  \end{split}
\end{equation*}
Isso nos garante um decaimento exponencial da probabilidade da média dos $X_i$ se desviar da esperança.

\begin{exercise}
  Aplique o método acima para variáveis $X_i$ \iid com distribuição $\Ber(1/2)$ e encontre $s(a)$ que otimize o decaimento da probabilidade $P\big[\sum_{i=1}^n X_i > (1/2 + a) n \big]$.
\end{exercise}

Poderíamos nos perguntar se a cota acima é suficientemente boa.
Talvez pudéssemos esperar um decaimento ainda melhor que exponencial.
Para responder a essa pergunta, vamos considerar o seguinte exemplo.
Sejam $(X_i)_{i \geq 1}$ variáveis \iid com $X_1 \distr \Ber(1/2)$.
Nesse caso temos por exemplo
\begin{equation}
  P\Big[ \big| \frac{X_1 + \dots + X_n}{n} - \frac 12 \big| \geq \frac 14\Big] \geq P[X_i = 1, \forall i \leq n] = 2^{-n}.
\end{equation}
Dessa forma, sabemos que não podemos esperar um decaimento melhor que exponencial, mesmo para variáveis bem simples (como Bernoulli) que satisfazem $\phi_X(s) < \infty$ para todo $s \in \mathbb{R}$.

Note que para variáveis com distribuição $\Ber(1/2)$, obtivemos acima cotas exponenciais em $n$ (superior e inferior), mas elas possuem expoentes diferentes.
Resta agora tentar entender qual é o expoente correto para o decaimento da probabilidade $P[X_1 + \dots + X_n \geq n(E(X_1) + a)]$, o que será feito na próxima seção.

\todosec{Tópico: Processos de ramificação}{fazer...}

\section{Princípio de Grandes Desvios}
\label{s:PGD}

A primeira tarefa nossa será otimizar a estimativa grosseira feita na seção anterior.
Essas estimativas são chamadas de \emph{estimativas de grandes desvios}, pois se referem a probabilidades que a média empírica de $X_i$ se desvie de sua esperança por um valor constante $a$.
Futuramente no curso estudaremos as probabilidades de que esse desvio seja de ordem $a_n \to 0$ que são chamados de \emph{desvios moderados} ou \emph{flutuações}, dependendo se a probabilidade de desvio converge a zero ou não.

\begin{theorem}[Princípio de Grandes Desvios - cota superior]
  \index{Principio@Princípio!de Grandes Desvios@de Grandes Desvios}
  \label{t:PGDleq}
  Consideramos variáveis aleatórias \iid $X_1, X_2, \dots$ tais que $\phi_{X_1}(s) < \infty$, para todo $s \in (-\delta, \delta)$.
  Então, para $a > 0$,
  \begin{equation}
    P\big[ X_1 + \dots + X_n \geq \big(m + a \big) n \big] \leq \ex{-\psi_{X_1}(m + a) n},
  \end{equation}
  onde $m = E(X_1)$ e
  \begin{equation}
    \psi_{X_1}(x) = \sup_{s \in \bbR} \big\{ xs - \log \big( \phi_{X_1}(s) \big) \big\}
  \end{equation}
  é chamada função taxa. \index{funcao@função!taxa}
  De jeito simetrico temos tambem
    \begin{equation}
    P\big[ X_1 + \dots + X_n \leq \big(m - a \big) n \big] \leq \ex{-\psi_{X_1}(m - a) n}.
  \end{equation}
\end{theorem}

Podemos reparar que
  \begin{equation}
    \psi_{X_1}(x) = \begin{cases}
                     \sup_{s \ge 0} \big\{ xs - \log \big( \phi_{X_1}(s) \big) \big\}, \text{ se } x\le m \\
                      \sup_{s \ge 0} \big\{ xs - \log \big( \phi_{X_1}(s) \big) \big\}, \text{ se } x\ge m.
                    \end{cases}
   \end{equation}
Isso e uma consequencia simples to fato que a função $s\mapsto xs - \log \big( \phi_{X_1}(s) \big)$ e concava e que a derivada em $0$ vale
$$x- \partial_s \big(\log  E[e^s X] \big)=x-m.$$ Então dependendo do sinal de $(x-m)$ o supremo e atingido a direito o a esqueirda de zero.



%Antes de provar o teorema, vamos fazer uma breve observação sobre como a função geradora de momentos se comporta com respeito a soma de constantes.
%Isso nos permitirá centrar as variáveis para nossas estimativas.
%
%\begin{lemma}
%  \label{l:phi_Xmaisb}
%  Seja $X$ uma variável aleatória tal que para algum $s_0 > 0$ tenhamos $\phi_{X}(s) < \infty$ para todo $s \in (-\delta, \delta)$.
%  Então
%  \begin{equation}
%    \log\big(\phi_{X - b}(s)\big) = \log\big(\phi_{X}(s)\big) -bs < \infty, \text{ para todo $s \leq s_0$.}
%  \end{equation}
%\end{lemma}

%\begin{proof}
%  Basta observar que
%  \begin{equation}
%    \phi_{X - b}(s) = E\big( \ex{s(X-b)} \big) = \ex{-sb} E\Big( \ex{sX}\Big) = \ex{-sb} \phi_X(s),
%  \end{equation}
%  e tomar logarítmos de ambos os lados para obter o resultado.
%\end{proof}

\begin{proof}
  Já sabemos que, para todo $s \geq 0$,
  \begin{equation}
    \begin{split}
      P\big[ X_1 & + \dots + X_n \geq \big(m + a \big) n \big] \leq \phi_{X_1}^n (s) \ex{-s (m + a) n}\\[1mm]
      & = \ex{ \log \big( \phi_{X_1}(s)\big) n - s(m + a) n}\\
      & = \ex{ - \big( (m + a)s - \log \big( \phi_{X_1}(s)\big) \big) n}\\
%      & \overset{\text{Lema}~\ref{l:phi_Xmaisb}\;\;}= & \ex{ \log \big( \phi_{X_1 - m}(s) \big) n - s a n}\\
%      & = & \ex{ - \big(as -\log \big( \phi_{X_1 - m}(s) \big) \big) n}
    \end{split}
  \end{equation}
  O que termina a prova do teorema se tomamos o ínfimo em $s \geq 0$.
\end{proof}

\begin{exercise}
  Calcule $\psi_X(a)$ quando $X$ é distribuída como $\Ber(p)$, $U_{[0,1]}$ e $\Exp(\lambda)$.
\end{exercise}

\begin{exercise}
  Na Nova Caledônia, temos $k$ habitantes.
  Seja $f:\{1, \dots, k\} \to \{0,1\}$ uma função que indica a intenção de voto de cada cidadão.
  Mais precisamente, para cada habitante $i \in \{1, \dots, k\}$, se $f(i) = 0$, então $i$ vota no candidato $0$, enquanto se $f(i) = 1$, o cidadão $i$ vota no candidato $1$.
  Para estimar o número $k_1 = \# f^{-1}(\{1\})$ de pessoas que votam em $1$, nós escolhemos variáveis aleatórias $Y_i$ i.i.d. com distribuição uniforme em $\{1, \dots, k\}$ e queremos estimar
  \begin{equation}
    \text{Err}_n(\epsilon) = P \Big[ \Big| \frac{1}{n} \sum_{i=1}^n f(Y_i) - \frac{k_1}{k} \Big| > \epsilon \Big].
  \end{equation}
  Sabendo que $k$ é par e $k_1 = k/2$, então
  \begin{enumerate}[\quad a)]
  \item use o método do segundo momento para obter um $n$ tal que $\text{Err}_{n}(0.01) < 0.02$ e um $n$ tal que $\text{Err}_{n}(0.01) < 10^{-12}$,
  \item use o método do momento exponencial para obter resolver o ítem acima.
  \end{enumerate}
  Compare os quatro resultados obtidos acima.
\end{exercise}

Vamos agora tomar um exemplo concreto para análise.
Sejam $X_1, X_2, \dots$ variáveis aleatórias \iid com distribuição $\Ber(1/2)$, donde
\begin{equation}
  \phi_{X_1}(s) = \frac{1}{2} (1 + e^s) \quad \text{e} \quad \psi_{X_1}(x) = \sup_{s \geq 0} \{xs - \log(1 + e^s) + \log(2) \}.
\end{equation}
Um cálculo simples nos mostra que, se $x < 1$, o mínimo acima é atingido no único ponto $s_{\text{max}} = \log(\tfrac{x}{1-x})$.
Portanto, podemos concluir do Teorema~\ref{t:PGDleq} que
\begin{equation}
  \begin{split}
    P[X_1 + \dots & + X_n > 1/2 + a] \leq \ex{- \psi_{X_1}(s_{\text{max}})n}\\
    & = \exp\Big\{-n \Big(b \log(b) + (1-b)\log(1-b) + \log(2) \Big)\Big\}
  \end{split}
\end{equation}
Note que $P[X_1 + \dots + X_n = n] = 2^{-n} = \ex{-\log(2)n} = \ex{-\psi_{X_1}(1-)n}$.
Isso nos dá um forte indício de que talvez nossas cotas superiores não estejam tão longe de ser precisas.
Para confirmar essa hipótese, precisamos obter cotas inferiores parecidas.

\begin{figure}[!ht]
  \centering
  \begin{tikzpicture}[scale=3]
    \draw[->] (-0.2,0) -- (1.2,0) node[right] {$b$};
    \draw[-] (1,0.02) -- (1,-0.02) node[below] {$1$};
    \draw[-] (0.02,{ln(2)}) -- (-0.02,{ln(2)}) node[left] {$\log(2)$};
    \node[below left] at (0,0) {$0$};
    \draw[->] (0,-0.2) -- (0,1.1) node[above] {$\psi_{X}(b)$};
    \draw[domain=0.0001:0.9999,smooth,variable=\x,blue] plot ({\x},{\x*ln(\x) + (1 - \x)*ln(1 - \x) + ln(2)});
    \draw[->] (1.8,0) -- (3.2,0) node[right] {$b$};
    \draw[-] (3,0.02) -- (3,-0.02) node[below] {$1$};
    \node[below left] at (2,0) {$0$};
    \draw[->] (2,-0.2) -- (2,1.6) node[above] {$\psi_{X'}(b)$};
    \draw[domain=2.0001:2.9999,smooth,variable=\x,blue] plot ({\x},{(\x-2)*ln((\x - 2)/0.75) + (1 - \x + 2)*ln((1 - \x + 2)/(0.25))});
    \draw[-,dotted] (3,{ln(4/3)}) -- (1.98,{ln(4/3)}) node[left] {$\log(4/3)$};
    \draw[-] (2.02,{ln(4)}) -- (1.98,{ln(4)}) node[left] {$\log(4)$};
  \end{tikzpicture}
  \caption{Funções taxa $\psi_{X}(b)$ de uma variável $X$ com distribuição $\Ber(1/2)$, e $\psi_{X'}(b)$ de uma variável com distribuição $\Ber(3/4)$, para $b \in (0,1)$.}
\end{figure}

Antes de buscar cotas inferiores para as probabilidades de desvio, vamos estabelecer algumas propriedades da função $\psi_X(b)$.
Primeiramente, quando podemos dizer que o supremo na definição de $\psi_X$ é atingido em algum $s_{\text{max}}$?
Certamente, esse nem sempre é o caso, por exemplo se $X = m$ quase certamente, então $\phi_X(s) = e^{sm}$ e o supremo definindo $\psi_X(b)$ não é atingido se $b \neq m$.

\begin{lemma}
  \label{l:smax_PGD}
  Seja $X$ uma variável aleatória tal que $\phi_X(s) < \infty$ para todo $s \in (-\delta, \delta)$.
  Supondo $a \geq 0$ é tal que $P[X > m + a] > 0$, então existe $s_{\text{max}} \geq 0$ tal que
  \begin{equation}
    \psi_X(m + a) = (m + a)s_{\text{max}} - \log\big(\phi_X(s_\text{max})\big).
  \end{equation}
\end{lemma}

\begin{proof}
Para mostrar que o minimo e atingido e em ponto e suficiente mostrar que
$$ \lim_{s\to \infty} (m + a)s - \log\big(\phi_X(s)\big)=\infty,$$
pois a função e continua.
Por hipótese, existe $x > m + a$ tal que $p = P[X \geq x] > 0$, donde $\phi_X(s) \geq p e^{sx}$ e portanto podemos concluir por que
$$(m + a)s - \log\big(\phi_X(s)\big)\ge (x-m+a)s -\log p.$$

\end{proof}

\begin{lemma}
  Seja $X$ uma variável aleatória tal que $\phi_X(s) < \infty$ para todo $s \in (-\delta, \delta)$.
  Então o conjunto onde a função $\psi_X(s)$ é finita é um intervalo, na qual $\psi_X$ é convexa e portanto contínua.
\end{lemma}

\begin{proof}
  Primeiramente, supomos que $a < b$ são tais que $\psi_X(a)$ e $\psi_X(b)$ são finitas.
  Logo, para todo $c \in (a, b)$, temos que a função linear $cs$ é menor ou igual a $as \vee bs$, daí
  \begin{equation}
    \begin{split}
      \psi_X(c) &= \sup_{s \geq 0} \{cs - \log(\phi_X(s))\} \leq  \sup_{s \geq 0} \{(as \vee bs) - \log(\phi_X(s))\}\\
      & \leq \sup_{s \geq 0} \{as - \log(\phi_X(s))\} \vee \sup_{s \geq 0} \{bs - \log(\phi_X(s))\} < \infty.
    \end{split}
  \end{equation}
  Para mostrar que $\psi_X$ é convexa, observe que $\psi_X(x)$ é dada pelo supremo (para $s \geq 0$) das funções afins $x \mapsto xs - \psi_X(s)$.
  Como o supremo de funções convexas é também convexo, obtemos o enunciado do lemma.
\end{proof}


\begin{exercise}
  Suponha que se $\phi_{X}(s)$ é finita para todo $s \in (-\delta, \delta)$ e mostre que as seguintes afirmações valem.
  \begin{enumerate}[\quad a)]
  \item A função $\psi_{X}(s)$ é não negativa, semi-contínua inferior e convexa em seu domínio.
  \item $\psi_X(a)$ se anula somente em $a = m$.
  \end{enumerate}
\end{exercise}

Buscaremos agora cotas inferiores para a probabilidade de obter um grande desvio.
Gostaríamos que essas estimativas fossem o mais próximas possíveis das estimativas superiores obtidas acima.
Certamente não podemos obter algo como
\begin{equation}
  \label{e:PGDgeq_falso}
  `` P\big[ X_1 + \dots + X_n \geq \big(m + a \big) n \big] \geq \exp\{-\psi_{X_1}(a) n\} ",
\end{equation}
pois senão isso nos daria uma igualdade o que é impossível, pois perdemos um pouco de precisão ao utilizar a desigualdade de Markov na cota superior.

Contudo, gostaríamos de entender se ao menos o expoente $\psi_{X_1}(a)$ na cota superior também possui algum papel na cota inferior.
Isso é confirmado no seguinte resultado.

\begin{theorem}[Princípio de Grandes Desvios - cota inferior]
  \index{Principio de Grandes Desvios@Princípio de Grandes Desvios}
  \label{t:PGDgeq}
  Sejam $X_1, X_2, \dots$ variáveis aleatórias \iid com $\phi_{X_1}(s) < \infty$, para todo $s \in \mathbb{R}$.
  Então, para todo $a > 0$,
  \begin{equation}
    \liminf_{n \to \infty} \frac{1}{n} \log P\big[ X_1 + \dots + X_n \geq \big(m + a \big) n \big] \geq -\psi_{X_1}(m + a),
  \end{equation}
  onde novamente $m = E(X_1)$ e $\psi_{X_1}(x)$ é definida como no Teorema~\ref{t:PGDleq}.
  De jeito analogo
    \begin{equation}
    \liminf_{n \to \infty} \frac{1}{n} \log P\big[ X_1 + \dots + X_n \leq \big(m - a \big) n \big] \geq -\psi_{X_1}(m - a),
  \end{equation}
\end{theorem}

Note que o resultado do teorema acima é mais fraco que o que vemos na equação \eqref{e:PGDgeq_falso}, mas mostra que $\psi_{X_1}(a)$ é realmente o expoente correto no decaimento da probabilidade de grandes desvios.

Um corolário dos Teoremas~\ref{t:PGDleq} e \ref{t:PGDgeq} é o seguinte

\begin{corollary}
  Se $X_1, X_2, \dots$ variáveis aleatórias \iid com $\phi_{X_1}(s) < \infty$, para todo $s \in \mathbb{R}$, então
  \begin{equation}
    \lim_{n \to \infty} \frac{1}{n} \log P\big[ X_1 + \dots + X_n \geq \big(m + a \big) n \big] = -\psi_{X_1}(m + a).
  \end{equation}
\end{corollary}

A idéia da prova é transformar a distribuição de $X_i$, usando uma exponencial como derivada de Radon-Nikodim.
Essa nova distribuição possuirá esperança maior que $m + a$, de forma que se tomamos a média de variáveis \iid $X'_1, \dots, X'_n$ distribuídas dessa forma, obteremos algo que se concentra acima de $m + a$.
Finalmente, o preço pago para que as variáveis $X_i$ se comportem como as $X'_i$ será aproximadamente $\exp\{-\psi_{X_1}(m + a)\}$, como desejado para nossa cota inferior.

\begin{proof}
  Primeiramente, consideraremos o caso $P[X_1 \leq m + a] = 1$, que se assemelha ao caso que analizamos acima $(\Ber(1/2) \leq 1)$.
  Nesse caso, temos
  \begin{equation*}
    \begin{split}
      P\big[ X_1 + \dots + X_n \geq \big(m + a \big) n \big] & = P[X_i = m + a, \text{ para todo $i \leq n$}]\\
      & = P[X_1 = m + a]^n.
    \end{split}
  \end{equation*}
  Donde o limite acima é igual a $\log(P[X_1 = m + a])$.
  Mas por outro lado,
  \begin{equation*}
    \begin{split}
      - \psi_{X_1}(m + a) & = \inf_{s \geq 0} \big\{ \log\big(E(\ex{s (X_1)})\big) - (m + a)s \big\} = \inf_{s \geq 0} \big\{ \log\big(E(\ex{s (X_1 - m - a)})\big) \big\}\\
      & \leq \liminf_{s \to \infty} \; \log\big(E(\ex{s (X_1 - m - a)})\big) = \log \big(P[X_1 = m + a]\big),
    \end{split}
  \end{equation*}
  pelo Teorema da Convergência Dominada, demonstrando o teorema nesse caso especial.

  Suponhamos agora que $P[X_1 > m + a] > 0$, o que implica que para $b > m + a$ suficientemente próximo de $m + a$, temos $P[X_1 > b] > 0$.
  Observe que basta mostrar que para todo $b > a$ satisfazendo $P[X_1 > b] > 0$ e para todo $\delta > 0$, temos
  \begin{equation}
    \label{e:PGD_perto_b}
    \liminf_n \frac{1}{n} \log \Big(P\Big[\frac{X_1 + \dots + X_n}{n} \in (b - \delta, b + \delta) \Big]\Big) \geq -\psi_{X_1}(b),
  \end{equation}
  pois a função $\psi_{X_1}(x)$ é convexa, portanto contínua.

  Vamos definir uma nova distribuição $\nu$ com derivada de Radon-Nikodim
  \begin{equation}
    \frac{\d \nu}{\d P_{X_1}} = \frac{1}{Z_\sigma} \ex{\sigma x}.
  \end{equation}
  Observamos primeiramente que o valor de $\sigma$ ainda não foi escolhido.
  Além disso após escolhido $\sigma$, teremos que calcular a constante de normalização $Z_{\sigma}$ de forma que $\nu$ seja uma probabilidade.

  Escolheremos $\sigma \geq 0$ como no Lema~\ref{l:smax_PGD}, isto é, tal que $\psi_{X_1}(b) = b\sigma - \log\big( \phi_{X_1}(\sigma) \big)$.
  Isso nos dá imediatamente que $Z_\sigma = E[\ex{\sigma X_1}] = \phi_{X_1}(\sigma)$
  por definição.

  Por diferenciabilidade de $\phi_{X_1}$, o máximo deve ser assumido em um ponto de derivada zero para a função $\psi_{X_1}$, ou seja
  \begin{equation}
    b = \frac{\phi_{X_1}'(\sigma)}{\phi_{X_1}(\sigma)} \overset{\text{Prop.~\ref{p:propried_phi}}}= \frac{E(X \ex{\sigma X})}{E(\ex{\sigma X})} = \frac{E(X \ex{\sigma X})}{Z_\sigma} = \int x \nu(\d x).
  \end{equation}
  Isso implica que se uma variável aleatória tem distribuição $\nu$, sua esperança é $b$.
  É possível verificar que uma tal variável aleatória $X'$ satisfaz obrigatoriamente $\phi_{X'}(s) < \infty$ para todo $s \geq 0$, donde $X' \in \mathcal{L}^p$ para todo $p > 1$.

  Como prometido, consideramos variáveis $X_1', X_2', \dots$ \iid com distribuição $\nu$.
  Pela lei fraca dos grandes números, para qualquer $\delta > 0$,
  \begin{equation}
    \lim_n P\Big[ \frac{X_1' + \dots + X_n'}{n} \in (b-\delta,b+\delta) \Big] = 1.
  \end{equation}

  Finalmente vamos relacionar essa probabilidade à probabilidade definida em termos de $X_i$, na qual estamos interessados.
  \begin{equation*}
    \begin{split}
      P\Big[ & \frac{X_1 + \dots + X_n}{n} \in (b-\delta, b+\delta) \Big] = \int_{x_i; \big| \tfrac{1}{n} \sum_{i \leq n} x_i - b\big| < \delta} \;\; \bigotimes_{i=1}^n (X_1 \circ P)(\d x_i)\\
      & = Z_\sigma^n \int_{x_i; \big| \tfrac{1}{n} \sum_{i \leq n} x_i - b \big| < \delta} \;\; \ex{-\sigma \textstyle{\sum_{i=1}^n x_i}} \bigotimes_{i=1}^n \nu(\d x_i)\\[2mm]
      & \geq Z_\sigma^n \exp\{-(b + \delta) \sigma n\} P\Big[ \frac{X_1' + \dots + X_n'}{n} \in (b-\delta,b+\delta) \Big].
    \end{split}
  \end{equation*}
  Tomando o logarítmo, dividindo por $n$ e tomando o liminf quando $n$ vai a infinito, recuperamos
  \begin{equation}
    \begin{split}
      \lim_n \frac{1}{n} \log \Big(P\Big[ & \frac{X_1 + \dots + X_n}{n} \in (b - \delta,b +  \delta) \Big] \Big) \geq \log(Z_\sigma) - (b + \delta) \sigma\\
      & = \log(\phi_{X_1}(\sigma)) - (b + \delta) \sigma = -\psi_{X_1}(\sigma) - \delta \sigma.
    \end{split}
  \end{equation}
  Como isso vale para todo $\delta > 0$, provamos \eqref{e:PGD_perto_b} o que conclui a prova do teorema.
\end{proof}

\begin{exercise}
  Mostre o Teorema~\ref{t:PGDgeq} no caso em que $\phi_{X_1}(s) < \infty$, para todo $s \in (-\delta, \delta)$.
\end{exercise}

\newpage


\begin{topics}

  \section[Tópico: Funções características]{Tópico: Funções características \footnote{Somos gratos a Rangel Baldasso por escrever essa seção.}}

  Esta seção trata da função característica de uma variável aleatória, que pode ser vista como um análogo complexo da trasformada de Laplace, ou também como a transformada de Fourier de uma distribuição em $\mathbb{R}$.
  Vamos estudar suas principais propriedades e demonstrar que a função características determinam unicamente a distribuição da variável aleatória.

  \begin{definition}
    Dada uma variável aleatória $X$, a função característica de $X$, $\widebar{\phi}_{X}:\mathbb{R} \rightarrow \mathbb{C}$, é definida por
    \begin{equation}
      \widebar{\phi}_{X}(t)=\mathbb{E}(e^{itX}), \qquad t \in \mathbb{R}.
    \end{equation}
  \end{definition}

  Vamos começar estudando as propriedades básicas de $\widebar{\phi}_{X}$.

  \begin{exercise}
    Prove que a função $\widebar{\phi}_{X}$ é absolutamente contínua.
  \end{exercise}

  \begin{exercise}
    Suponha que $\mathbb{E}(|X|^{n}) < +\infty$. Prove que a função $\widebar{\phi}_{X}$ é $n$ vezes diferenciável em $t=0$ e que $\widebar{\phi}_{X}^{(n)}(0)=i^{n}\mathbb{E}(X^{n})$.
  \end{exercise}

  \begin{exercise}
    Se $X_{1}, X_{2}, \ldots, X_{n}$ são independentes e $a_{1}, a_{2}, \ldots, a_{n} \in \mathbb{R}$, então
    \begin{equation}
      \widebar{\phi}_{a_{1}X_{1}+a_{2}X_{2}+\cdots+a_{n}X_{n}}(t)=\widebar{\phi}_{X_{1}}(a_{1}t)\widebar{\phi}_{X_{2}}(a_{2}t)\cdots\widebar{\phi}_{X_{n}}(a_{n}t).
    \end{equation}
  \end{exercise}

  Como vamos ver agora, a função característica nos permite recuperar a distribuição de $X$:

  \begin{exercise}
    Use a seguinte igualdade
    \begin{equation}
      \lim_{T \rightarrow +\infty}\int_{0}^{T}\frac{\sin(tz)}{t}\,dz=\begin{cases}
        1 & \text{se } z > 0 \\
        0 & \text{se } z = 0 \\
        -1 & \text{se } x < 0 \\
      \end{cases}
    \end{equation}
    para provar que se $a<b$ são pontos de continuidade da função de distribuição de $X$, $F_{X}$, então
    \begin{equation}
      F_{X}(b)-F_{X}(a)=\lim_{T \rightarrow +\infty} \frac{1}{2\pi}\int_{-T}^{T}\frac{e^{-itb}-e^{-ita}}{-it}\widebar{\phi}_{X}(t)\,dt.
    \end{equation}
    Conclua que a distribuição de $X$ é determinada por $\widebar{\phi}_{X}$.
  \end{exercise}

  \par O próximo exercício consiste em calcular algumas funções características.

  \begin{exercise}
    Calcule as funções características das seguintes distribuições:
    \begin{itemize}
    \item[i.] $X \sim Ber(p)$;
    \item[ii.] $X \sim Poisson(\lambda)$;
    \item[iii.] $X \sim N(0,1)$. \textit{Dica: fixe $z \in \mathbb{R}$, calcule $\mathbb{E}(e^{zX})$ e use o Princípio da continuação analítica.}
    \end{itemize}
  \end{exercise}
\end{topics}


\section{O Teorema Central do Limite}

Até o presente momento, já sabemos por exemplo que médias de variáveis aleatórias \iid, suficientemente regulares convergem para sua esperança quase certamente.
Vamos fazer contudo um experimento para visualizar esse fenômeno.

Nesse experimento, jogamos $100$ moedas e contamos quantas caras obtivemos.
Pelo que discutimos anteriormente, esperamos que esse número se encontre por volta de $50$, que é a esperança desta soma de variáveis \iid.
Vamos portanto repetir esse experimento mil vezes e observar quantas vezes obtemos algo próximo de $50$, veja Figura~\ref{f:histograma_normal}.

\begin{figure}[!ht]
  \centering
  \begin{tikzpicture}[scale=1]
    \draw[<->] (0, 3.5) -- (0,0) -- (8, 0);
    \foreach \x in {10, 20, 30, 40, 50, 60, 70}
    { \draw (.1*\x, -.1) -- (.1*\x, .1);
      \node[below] at (.1*\x, -.1) {$\x$};}
    \foreach \x in {50, 100, 150, 200, 250, 300}
    { \draw (-.1, .01*\x) -- (.1, .01*\x);
      \node[left] at (-.1, .01*\x) {$\x$};}
    \draw[fill, color=gray, opacity=.5] (3.40, 0) rectangle (3.72, 0.07);
    \draw[fill, color=gray, opacity=.5] (3.72, 0) rectangle (4.04, 0.27);
    \draw[fill, color=gray, opacity=.5] (4.04, 0) rectangle (4.36, 0.65);
    \draw[fill, color=gray, opacity=.5] (4.36, 0) rectangle (4.68, 1.61);
    \draw[fill, color=gray, opacity=.5] (4.68, 0) rectangle (5.00, 2.87);
    \draw[fill, color=gray, opacity=.5] (5.00, 0) rectangle (5.32, 2.00);
    \draw[fill, color=gray, opacity=.5] (5.32, 0) rectangle (5.64, 1.68);
    \draw[fill, color=gray, opacity=.5] (5.64, 0) rectangle (5.96, 0.58);
    \draw[fill, color=gray, opacity=.5] (5.96, 0) rectangle (6.28, 0.22);
    \draw[fill, color=gray, opacity=.5] (6.28, 0) rectangle (6.6, 0.05);
  \end{tikzpicture}
  \caption{Vários ensaios de uma variável $\Bin(100,0.5)$, pra ser mais preciso $1000$ ensaios. Cada barra representa o número de ensaios que caíram no intervalo determinado pela base da barra. Note que apesar dos experimentos se concentrarem em torno da média, alguns se afastam um pouco (obviamente pois o experimento é aleatório). Nessa seção estudaremos esses desvios espontâneos, que são chamados de flutuaçãoes. \index{flutuacoes@flutuações}}
  \label{f:histograma_normal}
\end{figure}

Nosso objetivo nessa seção será obter qual é o tamanho típico das flutuações em torno da média dessa soma de variáveis aleatórias.
Ao contrário do que fizemos ao estudar Grandes Desvios, nós agora estamos buscando flutuações menores, que acontecem espontaneamente e não com baixa probabilidade.

Note também que apesar de observarmos uma aleatoriedade na Figura~\ref{f:histograma_normal}, também notamos uma certa regularidade que muitas vezes é chamada de 'forma de sino' no histograma apresentado.

\subsection{A distribuição normal}

Começaremos estudando qual poderia ser uma possível forma limite para o histograma da Figura~\ref{f:histograma_normal}.

Como uma primeira tentativa, suponha que $\sum_{i=1}^\infty Z_i$ possui uma certa distribuição $\mu$ (veremos posteriormente que isso somente pode acontecer em casos triviais).
Mas se esse fosse o caso, poderíamos dividir a soma nos termos pares e ímpares $X = \sum_{i \text{ par}} Z_i$ e $Y = \sum_{i \text{ ímpar}} Z_i$.
Nesse caso teríamos $X$ e $Y$ independentes e também distribuídos como $\mu$ (pois são dados por uma soma que tem a mesma distribuição daquela que define $\mu$).

O seguinte lema mostra que isso somente pode acontecer na situação trivial em que $\mu = \delta_0$.

\begin{lemma}
  Sejam $X$ e $Y$ variáveis aleatórias em $\mathcal{L}^2$, \iid com distribuição $\mu$.
  Nesse caso, se $X + Y$ também tem distribuição $\mu$, então $\mu = \delta_0$.
\end{lemma}

\begin{proof}
  Sabemos que
  \begin{equation}
    \begin{split}
      E(X + Y) & = E(X) + E(Y) = 2 E(X) \text{ e}\\
      \Var(X + Y) & = \Var(X) + \Var(Y) = 2 \Var(X).
    \end{split}
  \end{equation}
  Mas como $X + Y$ tem a mesma distribuição de $X$, então $E(X) = 2 E(X)$ e $\Var(X) = 2 \Var(X)$, donde ambas são zero.
  Usando o método dos segundo momento, para todo $a > 0$,
  \begin{equation}
    P[|X| \geq a] \leq \frac{\Var(X)}{a^2} = 0,
  \end{equation}
  terminando a prova de que $X = 0$ quase certamente.
\end{proof}

A intuição dessa prova é que quando somamos duas variáveis não determinísticas, a incerteza da soma (medida através da variância) tende a aumentar.
Dessa forma não podemos obter a mesma distribuição após a soma.

Mas existe uma maneira simples de tornar esse problema interessante novamente.
Digamos que $X$ e $Y$ pertencem a $\mathcal{L}^2$ e são i.i.d.
Então
\begin{equation}
  \Var\Big(\frac{X + Y}{\sqrt{2}}\Big) = 2 \Var\Big( \frac X{\sqrt{2}} \Big) = \Var(X).
\end{equation}
Então podemos nos perguntar se

\begin{question}
  \label{q:ponto_fixo_soma}
  Existe alguma distribuição não trivial $\mu$ em $\mathcal{L}^2$ tal que, se $X$ e $Y$ são independentes e distribuídas de acordo com $\mu$, temos
  \begin{equation}
    \frac{X + Y}{\sqrt{2}} \distr \mu \; ?
  \end{equation}
  Pelo menos sabemos agora que a variância não se altera através dessa operação.
\end{question}

Ou em outras palavras, queremos saber se existe algum ponto fixo para o operador $\Gamma$ que toma uma distribuição $\mu$ em $\mathbb{R}$ e retorna
\begin{equation}
  \label{e:Gamma_operador}
  \Gamma(\mu) = \Big( \frac{X_1 + X_2}{\sqrt{2}} \Big) \circ \mu \otimes \mu.
\end{equation}


Para tentar responder a essa questão, vamos estudar mais a fundo qual é a distribuição da soma de duas variáveis aleatórias independentes.
Para isso, considere a distribuição $(X,Y) \circ P$ do par, que coincide com $\mu \otimes \mu$, nos dando
\begin{equation}
  P\Big[ \frac{X + Y}{\sqrt{2}} \leq z \Big] = \mu \otimes \mu \big( \big\{(x, y); \tfrac{x + y}{\sqrt{2}} \leq z \big\} \big).
\end{equation}

Note também que a transformação linear $(x,y) \mapsto \tfrac{1}{\sqrt{2}}\big(x + y, x - y\big)$ é uma rotação rígida em $\mathbb{R}^2$, o que nos motiva a propor a pergunta mais simples.

\begin{question}
  Existe alguma distribuição não trivial $\mu$ em $\mathcal{L}^2$ tal que, se $X$ e $Y$ são independentes e distribuídas de acordo com $\mu$, a distribuição do par $(X,Y)$ é invariante por rotações?
\end{question}

Ainda estamos numa busca não rigorosa de tal distribuição, então vamos supor algumas outras propriedades, como por exemplo que $\mu$ seja absolutamente contínua com respeito a Lebesgue, isto é $\d \mu = f(x) \d x$.
Nesse caso, já vimos que $(X, Y) \distr f(x) f(y) \d x \d y$ e no fundo estamos procurando uma função $f$ tal que
\begin{equation}
  f(x) f(y) = h(x^2 + y^2), \text{ para todo $x, y \in \mathbb{R}$ e alguma $h: \mathbb{R}_+ \to \mathbb{R}_+$.}
\end{equation}
Para trasformar o produto $f(x) f(y)$ em uma soma, definimos $g = \log f$ e $k = \log h$ e o que gostaríamos que acontecesse é $g(x) + g(y) = k(x^2 + y^2)$.
Como ainda não estamos preocupados com unicidade de $\mu$ e apenas com a existência, já podemos encontrar nossa resposta para nossa pergunta, escolhendo uma função quadrática, tal como $g(x) = \alpha x^2 - \beta$.

Mas temos ainda que cuidar para que $f(x) = \ex{\alpha x^2 - \beta}$ seja uma densidade, ou seja $\int f \d x = 1$.
Para isso, precisamos que $\alpha$ seja negativo e, fixado $\alpha$, o valor de $\beta$ já estará determinado por normalização.
Tudo isso motiva finalmente a seguinte definição.

\begin{definition}
  Dizemos que $X$ tem distibuição normal canônica, se \index{distribuicao@distribuição!normal}
  \begin{equation}
    \label{e:normal_canonica}
    X \distr \frac{1}{\sqrt{2 \pi}} \exp \big\{-x^2/2\big\} \d x.
  \end{equation}
  Além disso, para $m \in \mathbb{R}$ e $\sigma \geq 0$, dizemos que $Y \distr \mathcal{N}(m, \sigma^2)$ se $Y$ tem a mesma distribuição de $\sigma X + m$, onde $X$ tem distribuição normal canônica $\mathcal{N}(0, 1)$. Note que $\mathcal{N}(m, 0) = \delta_m$.
  Muitas vezes chamamos essa distribuição de gaussiana, obviamente em homenagem a Gauss.
\end{definition}


Vamos rapidamente observar que a definição acima realmente descreve uma distribuição de probabilidade, ou seja que a integral dessa densidade é um.
Para tanto, vamos usar um truque conhecido, que consiste em retornar ao plano.
Obviamente,
\begin{equation}
  \begin{split}
    \Big(\int \exp \big\{-x^2/2\big\} \d x\Big)^2 & = \int \int \exp \big\{-(x^2 + y^2)/2\big\} \d x \d y\\
    & = \int_0^{2 \pi} \int_0^\infty \exp \{ - r^2 / 2 \} r \d r \d \theta \overset{2 s \; = \; r^2}= 2 \pi.
  \end{split}
\end{equation}
Donde a constante em \eqref{e:normal_canonica} está de fato correta.

\begin{exercise}
  Mostre que a distribuição $\mathcal{N}(m, \sigma^2)$, tem densidade
  \begin{equation}
    \frac{1}{\sigma \sqrt{2 \pi}} \ex{-(x - m)^2/(2 \sigma^2)}.
  \end{equation}
\end{exercise}

\begin{exercise}
  Mostre que $Y \distr \mathcal{N}(m, \sigma^2)$ tem esperança $m$ e variância $\sigma^2$.
\end{exercise}

Para confirmar que de fato as distribuições normais se comportam bem com respeito a somas independentes, apresentamos o seguinte resultado.

\begin{proposition}
  \label{p:soma_normais}
  Se $X \distr \mathcal{N}(m, \sigma^2)$ e $Y \distr \mathcal{N}(\bar{m}, \bar{\sigma}^2)$ são independentes, então $X + Y$ tem distribuição $\mathcal{N}(m + \bar{m}, \sigma^2 + \bar{\sigma}^2)$.
  Em particular, $\mu$ é um ponto fixo do operador $\Gamma$ definido em \eqref{e:Gamma_operador}.
\end{proposition}

\begin{proof}
  O caso em que $\sigma$ ou $\bar{\sigma}$ se anulam é trivial, portanto vamos considerar que ambas são positivas.
  Não é difícil ver que podemos também supor que $m = \bar{m} = 0$.
  Podemos então calcular
  \begin{equation}
    P[X + Y \leq a] = P[\sigma W + \bar{\sigma} Z \leq a],
  \end{equation}
  onde $W$ e $Z$ são independentes com distribuição $\mathcal{N}(0,1)$.
  Assim, a probabilidade acima pode ser escrita como
  \begin{equation}
    \label{e:soma_normal}
    \mathcal{N}(0,1) \otimes \mathcal{N}(0,1) \Big( \big\{ (w,z) \in \mathbb{R}^2; \sigma w + \bar{\sigma} z \leq a \big\} \Big).
  \end{equation}
  Agora aplicaremos a rotação rígida $A: \mathbb{R}^2 \to \mathbb{R}^2$ dada por
  \begin{equation}
    A(w,z) = \frac{1}{\sqrt{\sigma^2 + \bar{\sigma}^2}} \big( \sigma w + \bar{\sigma} z, \bar{\sigma} w - \sigma z \big).
  \end{equation}

  Como sabemos que a densidade $f$ de $(W,Z)$ é invariante por $A$, ou seja $f \circ A = f$, então podemos escrever \eqref{e:soma_normal} como
  \begin{equation*}
    \begin{split}
      \mathcal{N}(0,1) & \otimes \mathcal{N}(0,1) \Big( A \big(\big\{ (w,z) \in \mathbb{R}^2; \sigma w + \bar{\sigma} z \leq a \big\} \big) \Big)\\
      & = \mathcal{N}(0,1) \otimes \mathcal{N}(0,1) \Big( \Big\{(w,z); \frac{1}{\sqrt{\sigma^2 + \bar{\sigma}^2}}w \leq a \Big\} \Big)\\
      & = \mathcal{N}(0,1) \big( (-\infty, a \sqrt{\sigma^2 + \bar{\sigma}^2} \big] \big) = \mathcal{N}(0,\sigma^2 + \bar{\sigma}^2) \big( (-\infty, a \big] \big),
    \end{split}
  \end{equation*}
  terminando a prova da proposição.
\end{proof}

Podemos obter um corolário interessante sobre a soma de normais i.i.d.
\begin{corollary}
  \label{c:normaliz_normais}
  Sejam $X_1, X_2, \dots$ variáveis \iid com distribuição $\mathcal{N}(m,\sigma^2)$, então
  \begin{equation}
    X_1 + \dots + X_n \distr \mathcal{N}(nm, n \sigma^2).
  \end{equation}
  Como consequência
  \begin{equation}
    \frac{\sum_{i=1}^n X_i - n E(X_1)}{\sigma \sqrt{n}} \distr \mathcal{N}(0,1).
  \end{equation}
\end{corollary}

Lembrando da Lei dos Grandes Números, se dividimos a soma dos $X_i - E(X_i)$ por $n$, essa fração vai a zero quase certamente.
O que concluímos acima é que ao dividir por $\sqrt{n}$ obtemos um limite não trivial (nem zero, nem infinito) e aleatório (não determinístico).

Mais uma observação curiosa: nossa motivação para a definição da distribuição normal passou por invariância por rotações e podemos extender essa invariância para $n$ normais independentes.
Note que somar as coordenadas canônicas é equivalente a tomar o produdo escalar com o vetor $(1,1,\dots,1)$, que tem norma euclideana $\sqrt{n}$.

Uma outra maneira de entender o corolário acima é que a normal é um ponto fixo da operação seguinte
\begin{enumerate}[\quad a)]
\item tome uma distribuição $\mu \in \mathcal{L}^2$,
\item considere $X_1, \dots, X_n$ \iid com distribuição $\mu$ e
\item retorne a distribuição de
  \begin{equation}
    \frac{X_1 + \dots + X_n - n E(X_1)}{\sqrt{n}}.
  \end{equation}
\end{enumerate}

Na Questão~\ref{q:ponto_fixo_soma}, nos perguntamos quais seriam os outros possíveis pontos fixos de $\Gamma$ e isso será considerado depois.
Mas uma outra questão bastante importante é se o ponto fixo $\mathcal{N}(0,1)$ é atrator, ou seja se começando com outras distribuições poderíamos nos aproximar de $\mathcal{N}(0,1)$ à medida que iteramos $\Gamma$.

Isso é estudado no Teorema Central do Limite (TCL) que provaremos posteriormente.
Mas antes, precisamos desenvolver uma boa definição de convergência para distribuições, ou em outras palavras definir uma topologia.
Esse será o nosso próximo tópico.

\subsection{Convergência fraca}

Em muitos casos é importante termos bem definida uma noção de convergência de medidas de probabilidade.
Supondo por exemplo no espaço mensurável $(E,\mathcal{A})$, tenhamos uma sequência de probabilidades $\mu_n$ e gostaríamos de saber se ela converge a uma determinada $\mu$.

Um candidato natural para dara sentido a essa convergência poderia se a distância de variação total entre duas medidas
\begin{equation}
  d_{\VT}(\mu,\nu) = \sup_{A \in \mathcal{A}} |\mu(A) - \nu(A)|.
\end{equation}
Não é difícil mostrar que a definição acima induz uma métrica, mas ela possui alguns problemas que descreveremos a seguir.

\begin{exercise}
  Mostre que $d_{\VT}$ define uma métrica.
\end{exercise}

\begin{exercise}
  Sejam $\mu$ e $\nu$ absolutamente contínuas com respeito a uma medida fixa $\eta$, tendo densidades $\rho$ e $\pi$ respectivamente.
  Encontre uma fórmula para $d_{\VT}(\mu, \nu)$ em termos das densidades.
  Essa fórmula nos remete a qual distância entre funções?
\end{exercise}

Digamos que o espaço amostral $E$ já seja provido de uma métrica $d$ e $\mathcal{A}$ seja a $\sigma$-álgebra dos borelianos em $E$.
Qualquer que seja a noção de convergência que iremos considerar, gostaríamos de dizer que $\delta_{x_n}$ converge a $\delta_x$ sempre que $x_n \to x$ em $E$.
Esse porém não é o caso para $d_{\VT}$, pois se $x_n \neq x$ para todo $n$ e $\{x\} \in \mathcal{A}$, teríamos
\begin{equation}
  d_{\VT}(\delta_{x_n}, \delta_x) \geq |\delta_{x_n}(\{x\}) - \delta_{x}(\{x\}) | = |0 - 1| = 1.
\end{equation}

Aqueles que já viram o conceito de convergência fraca acharão natural que a convergência de $\mu_n$ para $\mu$ seja definida em termos da convergência das integrais $\int f \d \mu_n$ para $\int f \d \mu$.
Porém, como mencionamos no exemplo das medidas $\delta_{x_n}$ acima, gostaríamos também de a convergência respeitasse a topologia original do espaço $E$, o que torna natural o seguinte conceito.

\begin{definition}
  Dizemos que uma sequência de medidas de probabilidade $\mu_n$ converge fracamente (ou converge em distribuição) para uma probabilidade $\mu$ se \index{convergencia@convergência!fraca}
  \begin{equation}
    \lim_{n \to \infty} \int f \d \mu_n = \int f \d \mu, \text{ para toda $f:E \to \mathbb{R}$ contínua e limitada.}
  \end{equation}
  Essa convergência muitas vezes é denotada por $\mu_n \Rightarrow \mu$.
\end{definition}

Essa definição fica ainda mais natural para aqueles que conhecem o Teorema da Representação de Riesz.
Com isso em mente, podemos relacionar a convergência em distribuição com a convergência fraca-$\star$ no espaço de medidas finitas.

\begin{exercise}
  Mostre que em $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$, temos que $\tfrac{1}{n} \sum_{i=1}^n \delta_{i/n} \Rightarrow U_{[0,1]}$.
\end{exercise}

\begin{exercise}
  Considere a função $\phi$ do espaço de medidas em $([0,1], \mathcal{B}([0,1]))$ nele mesmo, dada por:
  \begin{equation}
    \phi(\mu)(A) = \tfrac{1}{2} \big( \mu(3A) + \mu(3A - 2) \big).
  \end{equation}
  Identifique o limite em distribuição de $\phi^{(n)}(\delta_0)$.
  Mostre que
  \begin{enumerate}[\quad a)]
  \item a função de distribuição acumulada associada ao limite é contínua,
  \item o limite não é absolutamente contínuo com respeito à medida de Lebesgue.
  \end{enumerate}
\end{exercise}

\begin{exercise}
  Sejam $X_1, X_2, \dots$ i.i.d. distribuidas como $\text{Exp}(1)$ e defina
  \begin{equation}
    M_n = \max_{i = 1, \dots, n} X_i.
  \end{equation}
  Mostre que $M_n - \log(n)$ converge fracamente e identifique o limite.
  Observe que não precisamos dividir $M_n - \log(n)$ por nada para obter a convergência.
\end{exercise}

Nós algumas vezes denotamos $X_n \Rightarrow X$ quando $X_n$ e $X$ são elementos aleatórios de $(\Omega, \mathcal{F}, P)$ para descrever a convergência fraca de suas respectivas distribuições.
Mais precisamente, $X_n \circ P \Rightarrow X \circ P$.

\subsection{Convergência fraca em \texorpdfstring{$\mathbb{R}$}{R}}

No caso especial em que $E = \mathbb{R}$, temos vários outras maneiras de caracterizar convergência em distribuição.
A primeira é dada pela seguinte

\begin{proposition}
  \label{p:conv_distr_suave}
  Se $\int g \d \mu_n$ converge para $\int g \d \mu$ para toda $g \in C^3$ limitada e com as três primeiras derivadas limitadas, então $\mu_n \Rightarrow \mu$.
\end{proposition}

\begin{proof}
  Primeiramente, vamos ver que podemos nos concentrar em um conjunto compacto da reta.

  Para isso fixe um $\varepsilon > 0$ e tome $M'$ tal que $\mu\big( [-M', M'] \big) > 1 - \varepsilon / 3$.
  Tomando uma função $g$ satisfazendo as hipóteses do teorema e tal que
  \begin{equation}
    \1{[-M',M']} \leq g \leq \1{[-M'-1,M'+1]},
  \end{equation}
  concluimos que
  \begin{equation}
    \mu_n \big( [-M'-1, M'+1] \big) \geq 1 - \varepsilon/2,
  \end{equation}
  para todo $n$ suficientemente grande.
  Se tomamos $M \geq M'$ suficientemente grande, podemos obter a cota acima para todo $n$ (com $M$ no lugar de $M'+1$ e $\varepsilon$ no lugar de $\varepsilon/2$).

  Fixamos agora uma $f: \mathbb{R} \to \mathbb{R}$ contínua e limitada.
  Sabemos que é possível aproximar $f$ por uma função $g \in C^3$ de suporte compacto, com $\lVert g \rVert_\infty \leq 2 \lVert f \rVert_\infty$ e $|g - f| \leq \varepsilon/M$ uniformemente no intervalo $[-M,M]$.
  Essa $g$ certamente satisfaz as hipóteses do teorema.

  Portanto,
  \begin{equation*}
    \begin{split}
      \Big| \int f \d \mu_n - \int f \d \mu\Big| & \leq 2 \varepsilon \lVert f \rVert_\infty + \Big| \int_{-M}^M f \d \mu_n - \int_{-M}^M f \d \mu\Big|\\
      & \leq 2 \varepsilon \lVert f \rVert_\infty + \frac \varepsilon{M} 2 M + \Big| \int_{-M}^M g \d \mu_n - \int_{-M}^M g \d \mu\Big|\\
      & \leq 2 \varepsilon \lVert f \rVert_\infty + 2 \varepsilon + \Big| \int g \d \mu_n - \int \d \mu\Big|.
    \end{split}
  \end{equation*}
  Como o último termo converge a zero e $\varepsilon$ foi escolhido arbitrariamente, isso conclui a prova da proposição.
\end{proof}

\newpage

\subsection{O TCL para uma sequência i.i.d.}

\begin{theorem}[Teorema Central do Limite]
  \index{Teorema!Central do Limite}
  \label{t:tcl_iid}
  Considere em $(\Omega, \mathcal{F}, P)$, uma sequência $X_1, X_2, \dots$ de variáveis aleatórias \iid em $\mathcal{L}^3$.
  Nesse caso, se definimos $m = E(X_1)$ e $\sigma^2 = \Var(X_1)$, temos
  \begin{equation}
    \frac{\sum_{i=1}^n (X_i - m)}{\sigma \sqrt{n}} \Rightarrow \mathcal{N}(0,1).
  \end{equation}
\end{theorem}

\begin{proof}
  Primeiramente, observe que podemos supor que $m = 0$, pois de qualquer forma iremos subtrair a média da distribuição na qual nos interessamos.
  Uma outra observação importante é que podemos supor $\sigma = 1$, pois no caso geral de qualquer forma estamos somando $X_i/\sigma$ no enunciado.

  Como vimos na Proposição~\ref{p:conv_distr_suave}, basta mostrar a convergência das integrais de funções $g \in C^3$, que possuam todas as três primeiras derivadas limitadas.
  Considerando a função
  \begin{equation}
    \phi^n(x_1, \dots, x_n) := g\Big(\frac{x_1 + \dots + x_n}{\sqrt{n}} \Big),
  \end{equation}
  nos basta provar a convergência das sequências de números reais
  \begin{equation}
    \label{e:tcl_limite_phi}
    \lim_n \int \phi^n(X_1, \dots, X_n) \d P = \int g(s) \mathcal{N}(0,1)(\d s).
  \end{equation}

  Vale lembrar que no Corolário~\ref{c:normaliz_normais} já estabelecemos algo mais forte para variáveis normais.
  Mais precisamente, suponha que extendemos nosso espaço de probabilidade para $(\Omega', \mathcal{F}', P')$, onde exista uma sequência $Y_1, Y_2, \dots$ de variáveis aleatórias \iid com distribuição $\mathcal{N}(0,1)$ independente de $X_1, X_2, \dots$
  Então, para todo $n \geq 1$,
  \begin{equation}
    \int \phi^n(Y_1, \dots, Y_n) \d P' = \int g(s) \mathcal{N}(0,1) (\d s),
  \end{equation}
  o que tornaria o limite em \eqref{e:tcl_limite_phi} trivial para tais variáveis.
  A nossa estratégia será aproximar $\phi^n(X_1, \dots, X_n)$ por $\phi(Y_1, \dots, Y_n)$, e faremos isso trocando uma variável de cada vez.

  Para entender o que acontece quando trocamos uma das variáveis $X_i$ por $Y_i$, temos que expandir $g$ em série de potências, isto é, escrever
  \begin{equation}
    g(s) = g(s_0) + g'(s_0)(s - s_0) + g''(s_o)(s-s_0)^2/2 + r_{s_0}(s - s_0),
  \end{equation}
  onde $r_{s_0}(h)/h^3$ é limitada por $M$, uniformemente em $h$ e $s_0$ em consequência das nossas suposições sobre $g$.

  Denotando $z_i = (y_1, \dots, y_{i-1}, x_i, \dots x_n)$, $z_i^o := (y_1, \dots, y_{n-1}, 0, x_{n+1}, \dots, x_n)$ e $s_i^o = y_1 + \dots + y_{n-1} + x_{n+1} + \dots x_n$, temos
  \begin{equation}
    \phi^n(z_i) %& = \phi^n(z_i^o) + \frac{\partial \phi^n}{\partial x_i} (z_i^o) x_i + \frac{\partial^2 \phi^n}{\partial x_i^2} (z_i^o) \frac{x_i^2}{2} + r_z(x_i)\\
    = \phi^n(z_i^o) + g' \Big( \frac{s_i^o}{\sqrt{n}} \Big) \frac{x_i}{\sqrt{n}} + g'' \Big( \frac{s_i^o}{\sqrt{n}} \Big) \frac{x_i^2}{2n} + r_{\frac{s_i^o}{\sqrt{n}}} \Big( \frac{x_i}{\sqrt{n}} \Big),
  \end{equation}
  Nós propositalmente expandimos $\phi^n$ até ordem dois, pois $X_i$ e $Y_i$ possuem os mesmos momentos de ordem um ($m=0$) e dois ($\sigma^2=1$).

  Integrando os dois lados da igualdade acima com respeito a $Z_i \circ P$ (denotamos como antes, $Z_i = (Y_1, \dots, Y_{i-1}, X_i, \dots, X_n)$ e $Z_i^o$, $S_i^o$ analogamente), teremos
  \begin{equation}
    \int \phi^n(Z_i) \d P' = \int \phi^n(Z_i^o) \d P' + \frac{1}{2n} v_i + k_i,
  \end{equation}
  onde as quantidades $v$ e $k$, se escrevem como
  \begin{equation}
    v_i = \int g'' \Big( \frac{S_i^o}{\sqrt{n}} \Big) \d P' \quad \text{ e } \quad k_i = \int r_{S_i^o/\sqrt{n}} \Big(\frac{X_i}{\sqrt{n}}\Big) \d P'.
  \end{equation}
  Note que $v_i$ não depende de $X_i$ e que
  \begin{equation}
    |k_i| \leq \Big| \int \Big(\frac{X_i^3}{n^{3/2}}\Big) \Big(\frac{n^{3/2}}{X_i^3}\Big) r_{S_i^o/\sqrt{n}} \Big(\frac{X_i}{\sqrt{n}}\Big) \d P' \Big| \leq \frac{M}{n^{3/2}} E(|X_i^3|).
  \end{equation}

  As observações acima são o ponto mais importante da prova de que essa aproximação funciona e uma outra maneira de colocá-las é a seguinte.
  Como $X_i$ e $Y_i$ possuem os dois primeiros momentos iguais, os dois primeiros termos de Taylor coincidem após a integração (o primeiro se anula e o segundo é $v_i$ tanto para $X_i$ quanto para $Y_i$).
  O resto é de ordem muito pequena para influir no limite.

  De fato, se retiramos o termo $Y_i$ de $Z_{i+1}$, fazendo a mesma expansão que para $X_i$, obtemos
  \begin{equation}
    \int \phi^n(Z_{i+1}) \d P' = \int \phi^n(Z_i^o) \d P' + \frac{1}{2n} v_i + k'_i,
  \end{equation}
  com o termo de ordem superior $k'_i$ sendo definido exatamente como $k_i$, mas com $Y_i$ no lugar de $X_i$.

  Estamos prontos agora para a computação final
  \begin{equation*}
    \begin{split}
      \Big| \int \phi^n & (X_1, \dots, X_n) \d P - \int g(s) \mathcal{N}(0,1)(\d s) \Big|\\
      & = \Big| \int \phi^n(Z_0) \d P' - \int \phi^n(Z_n) \d P' \Big|\\
      & \leq \sum_{i=0}^{n-1} \Big| \int \phi^n(Z_{i}) \d P' - \int \phi^n(Z_{i+1}) \d P' \Big| = \sum_{i=0}^{n-1} |k_i - k'_i|\\
      & \leq n \frac{M}{n^{3/2}} \big(E(|X_1|^3) + E(|Y_1|^3) \big),
    \end{split}
  \end{equation*}
  que claramente converge a zero, provando o teorema.
\end{proof}

\begin{corollary}
  A $\mathcal{N}(0,1)$ é a única distribuição $\mu$ que possui esperança zero, variância $1$ e é tal que se $X, Y$ são \iid com distribuição $\mu$, então $(X + Y)/\sqrt{2}$ também possuem distribuição $\mu$.
  Em outras palavras, $\mathcal{N}(0, \sigma^2)$, para $\sigma \geq 0$, são os únicos pontos fixos de $\Gamma$ em $\mathcal{L}^3$.
\end{corollary}

\begin{proof}
  Usando a invariância enunciada acima, temos que
  \begin{equation}
    \frac{X_1 + \dots + X_{2^k}}{\sqrt{2^k}} \distr \mu.
  \end{equation}
  Mas pelo Teorema central do limite, a distribuição dessa combinação de $X_i$ deve convergir a $\mathcal{N}(0,1)$, logo temos $\mu = \mathcal{N}(0,1)$.
\end{proof}

Vamos terminar essa seção com uma aplicação do teorema acima.

\begin{exercise}
  Digamos que jogamos $100$ moedas honestas e independentes, como foi proposto no início da seção, obtendo finalmente uma variável aleatória $Y \distr \Bin(100, 1/2)$.
  Usando o \nameref{t:tcl_iid}, estime $P[Y \geq 55]$ usando uma aproximação por uma $\mathcal{N}(0,1)$.
  Calcule numericamente o valor real desta probabilidade e compare ambas as estimativas.
\end{exercise}

\todo{falar de Tao Vu, se os momentos batem a distrib de auto-val é proxima + funcao zeta.}

\todosec{Tópico: Mecânica estatística do gás ideal}{Mostrar a equivalência de ensembles.}

\todosec{Tópico: Funções características???}{funcoes caracteristicas e tomografia...}

\begin{topics}
  \section{Tópico: O Teorema de Portmanteau}

  O próximo resultado é bastante útil para provar convergência fraca, pois nos fornece uma coleção de equivalências muitas vezes mais fáceis de verificar.

  \begin{theorem}[Teorema de Portmanteau]
    \index{Teorema!de Portmanteau}
    \label{t:portmanteau}
    Sejam $(\mu_n)_{n \geq 1}$ e $\mu$ medidas de probabilidade em $(E, \mathcal{A})$.
    São equivalentes:
    \begin{enumerate}[\quad a)]
    \item[a)] $\mu_n \Rightarrow \mu$,
    \item[a')] $\int f \d \mu_n \to \int f \d \mu$, para toda $f$ unifmormemente contínua e limitada,
    \item[b)] $\limsup_n \mu_n(F) \leq \mu(F),$ para todo $F \subseteq E$ fechado,
    \item[b')] $\liminf_n \mu_n(G) \geq \mu(G),$ para todo $F \subseteq E$ aberto,
    \item[c)] $\lim_n \mu_n(A) = \mu(A),$ para todo $A \in \mathcal{A}$ com $\mu(\partial A) = 0$.
    \end{enumerate}
  \end{theorem}

  Para memorizar o teorema acima, é conveniente lembrar dos dois exemplos:
  \begin{enumerate}[\quad i)]
    \item se $x_n \to x$ com $x_n \neq x$, $F = \{x\}$ e $G = B(x, \delta) \setminus \{x\}$ temos, para $n$ grande,
      \begin{equation}
        \mu_n(F) = \mu(G) = 0 < 1 = \mu(F) = \mu_n(G),
      \end{equation}
    \item em $(\mathbb{R},\mathcal{B}(\mathbb{R}))$, seja $\mu_{2n} = \delta_n$ e $\mu_{2n+1} = \mu = \delta_0$.
      Obviamente $\mu_n$ não converge fracamente a $\mu$. Contudo, para todo $A \in \mathcal{B}(\mathbb{R})$,
      \begin{equation}
        \begin{split}
          \liminf_n \mu_n (A) & \leq \liminf_n \mu_{2n}(A) = \mu(A) \text{ e}\\
          \limsup_n \mu_n (A) & \geq \limsup_n \mu_{2n}(A) = \mu(A).
        \end{split}
      \end{equation}
  \end{enumerate}

  \begin{proof}[Prova do Teorema~\ref{t:portmanteau}]
    Obviamente, $(a \Rightarrow a')$, pois $a')$ somente supõe a convergência das integrais para funções $f$ que sejam uniformemente contínuas, portanto é um requisito mais fraco que $a)$.

    Observamos também que $(b \Leftrightarrow b')$.
    De fato, basta tomarmos complementos e observar a mudança nos sinais das desigualdades.

    Então, para a prova do teorema, basta mostrar que $(a' \Rightarrow b)$, $(b + b' \Rightarrow c)$ e $(c \Rightarrow a)$.

    Começamos com $(a' \Rightarrow b)$ e para tanto, consideramos $F \subseteq E$ fechado.
    Seja $\delta > 0$ e defina a função $f_\delta: E \to \mathbb{R}$ dada por
    \begin{equation}
      f_\delta (x) = \max \Big\{ 1 - \frac{d(x, F)}{\delta}, 0 \Big\}.
    \end{equation}
    Claramente, $f$ é uniformemente contínua e vale $\1{F} \leq f_\delta \leq \1{B(F,\delta)}$.
    Dessa desigualdade, temos $\limsup_n \mu_n(F) \leq \limsup_n \int f_\delta \d \mu_n = \int f_\delta \d \mu \leq \mu(B(F,\delta))$.
    Tomando agora o limite com $\delta \to 0$, obtemos $b)$ por continuidade da probabilidade $\mu$.

    Para mostrar $(b + b' \Rightarrow c)$, seja $A \in \mathcal{A}$ tal que $\mu(\partial A) = 0$.
    Nesse caso, sabemos que
    \begin{equation*}
      \begin{split}
        \limsup_n \mu_n(A) & \leq \limsup_n \mu_n(\bar A) \leq \mu (\bar A) = \mu (\mathring{A})\\
        & \leq \liminf \mu_n (\mathring{A}) \leq \liminf_n \mu_n (A),
      \end{split}
    \end{equation*}
    o que mostra o limite em $c)$.

    Finalmente, resta mostrar $(c \Rightarrow a)$ e, para tanto, consideramos uma função $f: E \to \mathbb{R}$ contínua e limitada.
    Digamos, com $\lVert f \rVert_\infty = M$.

    Sabemos que os conjuntos $\{f^{-1}(\{a\})\}_{a \in \mathbb{R}}$ são disjuntos, logo os conjuntos $f^{-1}(\{a\})$ podem ter medida $\mu$ positiva apenas para uma coleção enumerável de valores $a \in \mathbb{R}$.
    Obtemos assim uma coleção finita $b_0 < b_1 < \dots < b_k$, tal que
    \begin{equation}
      \begin{array}{c}
        b_0 < -M \text{ e } b_k > M, \quad b_{i+1} - b_i \leq \delta \text{ e}\\
        \mu\big(f^{-1} (\{b_i\}) \big) = 0 \text{ para todo $i \leq k$}.
      \end{array}
    \end{equation}

    \begin{figure}[!ht]
      \centering
      \begin{tikzpicture}[scale=3]
        \draw[->,gray,very thin] (-1,0) -- (1.4,0) node[right,black] {$x$};
        \draw[->,gray,very thin] (0.08,-0.2) -- (0.08,1.3) node[above,black] {$f(x)$};
        \draw[domain=-.8:1.2,smooth,variable=\x,blue] plot ({\x},{ 1/(1 + 10 * \x * \x) });
        \foreach \x in {5,7} { \draw[-,dashed,gray,very thin] ({-sqrt(1 / \x - 0.1)},0) -- ({-sqrt(1 / \x - 0.1)},0.1*\x) -- ({sqrt(1 / \x - 0.1)}, 0.1*\x) -- ({sqrt(1 / \x - 0.1)}, 0); }
        \foreach \x in {-1,1,3,5,7,9,11} { \draw[thick] (.06,0.1*\x) -- (.1, 0.1*\x); }
        \draw[thick] ({-sqrt(1 / 5 - 0.1)},0) -- ({-sqrt(1 / 7 - 0.1)},0);
        \draw[thick] ({sqrt(1 / 5 - 0.1)},0) -- ({sqrt(1 / 7 - 0.1)},0);
      \end{tikzpicture}
      \caption{Uma função contínua e limitada $f$, os pontos $b_i$ e um conjunto $A_i$.}
    \end{figure}

    Iremos aproximar $f$ por uma função da forma $f_\delta = \sum_{i} b_i \1_{A_i}$, onde os conjuntos $A_i = f^{-1}\big( [b_i, b_{i+1}) \big)$ são disjuntos.
    Obviamente $f_\delta \leq f \leq f_\delta + \delta$, donde
    \begin{equation*}
      \liminf \int f_\delta \d \mu_n \leq \liminf \int f \d \mu_n \leq \limsup \int f \d \mu_n \leq \liminf \int f_\delta \d \mu_n + \delta.
    \end{equation*}
    Mas como $\int f_\delta \d \mu_n = \sum_i b_i \mu_n (A_i)$, a prova estará concluida se mostrarmos que $\mu_n (A_i) \to \mu(A_i)$ para todo $i \leq k$.
    Isso segue de $d)$, pois $\partial A_i \subseteq f^{-1}(\{b_i, b_{i+1}\})$, que tem medida zero.
  \end{proof}

  \begin{exercise}
    Lembrando que em $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$, temos $\tfrac{1}{n} \sum_{i=1}^n \delta_{i/n} \Rightarrow U_{[0,1]}$, use o ítem $d)$ do Teorema~\ref{t:portmanteau} para dar uma caracterização dos conjuntos Riemann-mensuráveis.
    Mais precisamente, encontre os $A \subseteq \mathbb{R}$ tais que $\tfrac{1}{n} \sum_{i=1}^n \delta_{i/n}(A)$ converge para a medida de Lebesgue de $A$.
  \end{exercise}
\end{topics}

\todosec{Tópico: Análise de componentes principais}{variáveis gaussianas e principal component analysis...}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../Notas_de_aula"
%%% End:
