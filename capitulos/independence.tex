\documentclass[../main/Notas_de_aula.tex]{subfiles}

\begin{document}

\section{Independência}

Nossa intuição nos diz que quando jogamos duas moedas, o resultado de cada uma delas não deve depender um do outro.
Dessa forma, a probabilidade de obtermos um determinado resultado (como por exemplo duas caras) deve ser um quarto, ou seja meio vezes meio.

Em geral, definimos dois eventos como independentes da seguinte forma.

\begin{definition}
  Dizemos que dois eventos $A, B \in \mathcal{F}$, são \emph{independentes} \index{independencia@independência!de eventos} se
  \begin{equation}
    P(A \cap B) = P(A) P(B).
  \end{equation}
\end{definition}

\begin{example}
  Se $\Omega = \{1, \dots, 6\}$ é dotada da $\sigma$-álgebra das partes e e $P(A) = \#A/6$, então os eventos $A = [\omega \text{ é impar}]$ e $B = [\omega \geq 5]$ satisfazem
  \begin{equation}
    P(A \cap B) = P(\{5\}) = 1/6 = (1/2) (1/3) = P(A) P(B).
  \end{equation}
  Logo tais eventos são independentes.
\end{example}

\begin{exercise}
  Seja $\Omega = \{0,1\}^n$ com $P(A) = \#A/2^n$ e $X_i(\omega_1, \dots, \omega_n) = \omega_i$ para $i = 1, \dots, n$.
  Mostre que
  \begin{equation}
    P[X_i = a, X_j = b] = P[X_i = a] P[X_j = b],
  \end{equation}
  onde $[A, B]$ denota a interseção $[A] \cap [B]$.
\end{exercise}

\subsection{Coleções de eventos}

\todo{discutir a alternativa $I = \{1,\dots, k\}$ que é ruim (basta adicionar $\varnothing$ que qq coisa fica indep).}

\begin{definition}
  Sejam $A_1, A_2, \dots, A_k$ eventos.
  Dizemos que eles formam uma coleção independente \index{independencia@independência!de eventos} se para todo $I \subseteq \{1, \dots, k\}$ não vazio
  \begin{equation}
    P\big( \mcap\nolimits_{i \in I} A_i \big) =  \prod\limits_{i \in I} P(A_i).
  \end{equation}
\end{definition}

Vale observar que independência dois a dois não implica independência.
Mais precisamente
\begin{example}
  Seja $\Omega = \{1,2,3,4\}$ com $P(A) = \# A/4$ e sejam os seguintes eventos: $A_1 = \{1,2\}$, $A_2 = \{2,3\}$ e $A_3 = \{1,3\}$.
  Nesse caso,
  \begin{enumerate}[\quad a)]
  \item $P(A_i) = 1/2$ para $i = 1, 2, 3$,
  \item $P(A_i \cap A_j) = 1/4$ para todo $i \neq j$ mas
  \item $P(A_1 \cap A_2 \cap A_3) = 0 \neq 1/8 = P(A_1) P(A_2) P(A_3)$.
  \end{enumerate}
\end{example}

\begin{example}
  Sejam $n$ patos e $m$ caçadores.
  Cada caçador escolhe um pato aleatorea e uniformemente e atira (abatendo-o com probabilidade $p$).
  Seja $X = \# \{\text{patos vivos}\}$, que pode ter uma distribuição complicada de calcular, mas
  \begin{equation}
    \begin{split}
      E(X) &= E \Big( \sum_{i=1}^n \1_{[\text{pato $i$ vive}]} \Big) = \sum_{i=1}^n P[\text{pato $i$ vive}]\\
      &= n P[\text{pato $1$ vive}] = P\Big( \mcap_{j=1}^m [\text{caçador $j$ não mata pato $1$}] \Big)\\
      &= n P[\text{caçador $j$ não mata pato $1$}]^m = n \Big(1 - \frac{p}{n}\Big).
    \end{split}
  \end{equation}
  Observe que
  \begin{enumerate}[\quad a)]
  \item acima obtivemos uma igualdade e
  \item $[\text{pato $i$ vive}]$, $i = 1, \dots, n$ não são independentes.
  \end{enumerate}

  Finalmente estimamos (digamos para $n$ par)
  \begin{equation}
    \begin{split}
      & P[\text{patos para o jantar} \leq n/2] = P[X \geq n/2] \leq \frac{E(X)}{n/2}\\
      & \qquad = 2 \frac{n}{n} \Big( 1 - \frac{p}{n}\Big)^m \leq 2 \exp \{- \frac{pm}{n}\}.
    \end{split}
  \end{equation}
\end{example}

\begin{definition}
  Dizemos que uma coleção infinita de eventos $(A_n)_{n\ge 1}$ é independente \index{independencia@independência!de eventos} se toda sub-coleção finita de tais eventos forem independentes.
\end{definition}

\begin{lemma}
  Se $(A_n)_{n\ge 1}$ forma uma sequencia de eventos independentes, então
  \begin{equation}
    P\Big( \mcap_{i=1}^{\infty} A_i \Big) = \prod\limits_{i=1}^{\infty} P(A_i).
  \end{equation}
\end{lemma}

\begin{proof}
  De fato,
  \begin{equation*}
    P\Big( \mcap_{i=1}^{\infty} A_i \Big) = \lim_{n\to \infty} P\Big( \mcap_{i = 1}^n A_i \Big) = \lim_{n\to \infty} \prod\limits_{i=1}^n P(A_i) = \prod\limits_{i=1}^{\infty} P(A_i). \qedhere
  \end{equation*}
\end{proof}

\begin{exercise}
  Mostre que se $A \in \mathcal{F}$, então $\{B \in \mathcal{F}\, : \, B \text{ é independente de } A\}$ é um $\lambda$-sistema.
\end{exercise}

\begin{exercise}
  Mostre que se $B$ é independente de $A$ para todo $B \in \mathcal{B}$, com $\mathcal{B}$ um $\pi$-sistema, então $B$ é independente de $A$ para todo $B \in \sigma(\mathcal{B})$.
\end{exercise}

\subsection{Independência de \texorpdfstring{$\sigma$}{sigma}-álgebras}

\begin{definition}
  Dado um espaço de probabilidade $(\Omega,P,\mathcal{F})$ Dizemos que as $\sigma$-álgebra $\mathcal{F}_1,\dots,\mathcal{F}_n\subset \mathcal{F}$ são independentes \index{independencia@independência!de sigma-algebras@de $\sigma$-álgebras} se
  \begin{equation}
    \forall \mathcal{A}_1\in \mathcal{F}_1,\dots ,\mathcal{A}_n \in \mathcal{F}_n, \ P(\cap_{i=1}^n A_i)=\prod_{i=1}^n P(A_i).
  \end{equation}
  Nessa definição podemos tomar uma coleção infinita.
\end{definition}

\begin{exercise}
  Em um espaço produto $(\Omega_1 \times \Omega_2, \mathcal{F}_1 \otimes \mathcal{F}_2, P_1 \otimes P_2)$, podemos definir
  \begin{equation}
    \begin{split}
      \widebar{\mathcal{F}}_1 & = \{A \times \Omega_2 \, : \, A \in \mathcal{F}_1\},\\
      \widebar{\mathcal{F}}_2 & = \{\Omega_1 \times B \, : \, B \in \mathcal{F}_2\}.
    \end{split}
  \end{equation}
  Mostre que essas $\sigma$-álgebras são independentes.
\end{exercise}

Podemos extender esse conceito a elementos aleatórios, ou seja:
\begin{definition}
  Dizemos que $X_1, \dots, X_k$ são elementos aleatórios independentes \index{independencia@independência!de elementos} se as respectivas $\sigma$-álgebras $\sigma(X_1), \dots, \sigma(X_k)$ o forem.
\end{definition}

Quando $X_1, \dots, X_k$ são elementos aleatórios independentes e com a mesma distribuição, escrevemos que $X_i$ são \iid (independentes e identicamente distribuídos).

\begin{exercise}
  Com a notação do exercício anterior, mostre que as funções $X_i:\Omega_1 \times \Omega_2 \to \Omega_i$ dadas por
  \begin{equation}
    X_1(x,y) = x \text{ e } X_2 (x,y) = y,
  \end{equation}
  são elementos aleatórios e são independentes.
\end{exercise}

\begin{exercise}
  Mostre que as coordenadas canônicas do exercício anterior no caso $X_i: \mathbb{R}^2 \to \mathbb{R}$ não são independentes segundo a medida $U_{\mathbb{S}^1}$.
  Mas o são segundo $U_{[0,1]^2}$ (que é a medida de Lebesgue em $\mathbb{R}^2$ restrita a $[0,1]^2$).
\end{exercise}

\begin{exercise}
  Seja $\Omega = \{0,1\}^n$ com $P(A) = \#A/2^n$ e $X_i(\omega_1, \dots, \omega_n) = \omega_i$ para $i = 1, \dots, n$.
  Mostre que os $X_i$ são independentes.
\end{exercise}

\begin{exercise}
  Sejam $(X_i)_{i \geq 1}$ elementos aleatórios independentes tomando valores em espaços $(E_i)_{i \geq 1}$, respectivamente.
  Mostre que para funções mensuráveis $(f_i)_{i \geq 1}$ temos que $(f_i(X_i))_{i \geq 1}$ são independentes.
\end{exercise}

\begin{exercise}
  Mostre que se $X, Y$ são elementos aleatórios e se $X$ é constante quase certamente então $X$ e $Y$ são independentes.
\end{exercise}

\begin{exercise}
  Sejam $X$ e $Y$ variáveis aleatórias independentes com distribuição $\Exp(1)$, calcule a distribuição de
  \begin{enumerate}[\quad a)]
  \item $\min\{X,Y\}$ e
  \item $X + Y$.
  \end{enumerate}
\end{exercise}

\begin{exercise}
  Seja um espaço produto de medidas $(\Omega_1 \times \Omega_2, \mathcal{F}_1 \otimes \mathcal{F}_2, \mu_1 \otimes \mu_2)$ e defina a probabilidade $P$ atravéz de
  \begin{equation}
    \d P = \rho(x,y) \d (\mu_1 \otimes \mu_2).
  \end{equation}
  Mostre nesse caso que as coordenadas canônicas $X_1$ e $X_2$ são independentes se e somente se existem $\rho_1$ e $\rho_2$ em $\Omega_1$ e $\Omega_2$ respectivamente, tais que $\rho(x,y) = \rho_1(x) \rho_2(y)$ quase certamente com respeito a $\mu_1 \otimes \mu_2$.
\end{exercise}

\begin{exercise}
  Sejam $X, Y$ vari\'aveis aleat\'orias tais que
  \begin{equation}
    P[X \leq x, Y \leq y] =
    \begin{cases}
      0 & \quad \text{if $x < 0$,}\\
      (1-e^{-x}) \Big(\frac 12 + \frac 1\pi \tan^{-1} y \Big), & \quad \text{if $x \geq 0$}.
    \end{cases}
  \end{equation}
  \begin{enumerate}[\quad a)]
  \item Mostre que a distribui\c{c}\~ao conjunta $\mu_{(X,Y)}$ \'e
    absolutamente cont\'inua com rela\c{c}\~ao \`a medida de Lebesgue em
    $\mathbb{R}^2$.
  \item Mostre que $X$ e $Y$ s\~ao independentes.
  \end{enumerate}
\end{exercise}

\begin{exercise}
  \label{x:convolucao_densidade}
  Mostre que se $X, Y$ são variáveis aleatórias independentes com distribuições $X \distr f_X(x) \d x$ e $Y \distr f_Y(y) \d y$, então $X + Y$ tem distribuição absolutamente contínua com respeito a Lebesgue e
  \begin{equation}
    f_{X + Y}(z) = \int_{-\infty}^\infty f_Y(z - x) f_X(x) \d x.
  \end{equation}
\end{exercise}

\todo{mandar para depois de produtos infinitos?}

\begin{lemma}[Borel-Cantelli - segunda parte]
  Se $A_1, A_2, \dots \in \mathcal{F}$ são independentes e $p_i = P(A_i)$ satisfazem $\sum_i p_i = \infty$, então
  \begin{equation}
    P[A_i \text{ infinitas vezes}] = 1.
  \end{equation}
\end{lemma}

\begin{proof}
  Queremos mostrar que
  \begin{equation}
    P \Big( \big(\mcap_n \mcup_{i=n}^\infty A_i\big)^c \Big) = 0,
  \end{equation}
  mas
  \begin{equation}
    P \Big( \big(\mcap_n \mcup_{i=n}^\infty A_i\big)^c \Big) = P \Big(\mcup_n \mcap_{i=n}^\infty A_i^c \Big) \leq \sum\limits_n P \Big(\mcap_{i=n}^\infty A_i^c \Big).
  \end{equation}
  Logo basta mostrar que a probabilidade à direita é zero para todo $n$.
  Mas
  \begin{equation}
    \begin{split}
      P \Big(\mcap_{i=n}^\infty A_i^c \Big) & = \prod\limits_{i=n}^\infty P(A_i^c) = \prod\limits_{i=n}^\infty (1 - p_i)\\
      & \leq \prod\limits_{i=n}^\infty \exp\{-p_i\} = \exp\big\{- \sum_{i=n}^\infty p_i\big\} = 0.
    \end{split}
  \end{equation}
  Terminando a prova do lemma.
\end{proof}

\section{Espaços produto finito}

Dados espaços $\Omega_1, \dots, \Omega_n$ com suas respectivas $\sigma$-álgebras $\mathcal{F}_1, \dots, \mathcal{F}_n$, podemos definir o espaço mensurável produto $(\widebar{\Omega}, \widebar{\mathcal{F}})$ da seguinte forma
\begin{equation}
  \widebar{\Omega} = \prod_{i=1}^n \Omega_i \quad \text{e} \quad \widebar{\mathcal{F}} = \sigma \Big( \{
  A_1 \times \cdots \times A_n  \, : \,  \forall i \in \{1,\dots,n\},\ A_i \in \mathcal{F}_i \} \Big).
\end{equation}
Essa $\sigma$-álgebra e chamada de $\sigma$-álgebra produto e denotaremos ela por $\bigotimes_{i=1}^n \mathcal{F}_i$,
o $\mathcal{F}_1\otimes \mathcal{F}_2$ quando $n=2$.

\begin{proposition}
  Se $(\Omega_1, \mathcal{F}_1, P_1), \dots, (\Omega_n, \mathcal{F}_n, P_n)$ são espaços de probabilidade, então existe uma única probabilidade $\widebar{P}$ no espaço mensurável $(\widebar{\Omega}, \widebar{\mathcal{F}})$ tal que
  \begin{equation}
    \widebar{P}(A_1 \times \cdots \times A_n) = \prod_{i=1}^n P_i(A_i), \text{ para todos $A_i \in \mathcal{F}_i$, $i \leq n$.}
  \end{equation}
  Essa probabilidade é chamada probabilidade produto.
  Usaremos a notação $\bigotimes_{i=1}^n P_i$ o $P_1\otimes P_2 \otimes \dots \otimes P_n$.
\end{proposition}

\begin{proof}
  Teoria da Medida.
\end{proof}

Note que a unicidade do produto pode ser concluída por exemplo usando o Corolário~\ref{c:produto_e_unico}.

\begin{exercise}
  Mostre que o produto de $n$ cópias de $(\{0,1\}, \mathcal{P}(\{0,1\}), \Ber(1/2))$ é a distribuição uniforme em $\{0,1\}^n$.
\end{exercise}

\todosec{Tópico: Uma dinâmica em \texorpdfstring{$[0,1]$}{[0,1]}}{fazer dinâmica 2x mod 1 e relações Lebesgue[0,1] com produtos de bernoulli}

\begin{topics}

\section{Tópico: Lei dos pequenos números}

Nessa seção estudaremos como se comportam limites de algumas variáveis aleatórias bastante importantes, mas primeiramente, uma breve intuição.

Apesar de que descreveremos a nossa motivação a partir desse exemplo do estudo de um material radioativo, podemos encontrar aplicações com justificativas bastante semelhantes para outros problemas, como: chegada de carros em um sinal de trânsito, número de mutações em um gene, número de mortes por ano em uma faixa etária...

Digamos que estamos observando um material radioativo que esporadicamente emite fótons que podemos detectar atravéz de um aparelho.
A razão dessas emissões pode ser aproximada pelo seguinte modelo.
Na amostra temos um número $n$ grande de átomos instáveis ($n \sim 10^{23}$) e em um determinado tempo de observação, cada um deles tem probabilidade muito baixa de decair emitindo um fóton (digamos $p \sim 10^{-23}$).
Nesse caso, supondo que todos decidam emitir de maneira independente, temos para $p \in [0,1]$,
\begin{equation}
  \label{e:Poisson_setup}
  \Omega_n = \{0,1\}^n, \quad \mathcal{F}_n = \mathcal{P}(\Omega) \quad \text{e} \quad P_p = \otimes_{i=1}^n Ber(p).
\end{equation}
Dessa forma, o número total de emissões observadas para $\omega = (\omega_1, \dots, \omega_n) \in \Omega$ é
\begin{equation}
  \label{e:Xn_Poisson}
  X_n(\omega) = \sum_{i=1}^n \omega_i.
\end{equation}
E gostaríamos de entender como se comporta essa distribuição, que nada mais é que $\Bin(n,p)$.

Uma primeira tentativa seria modelar esse processo dizendo que o número de átomos $n$ é tão grande, que somente estamos interessados no comportamento assimtótico quando $n$ vai para infinito.
Mas para manter o número de emissões sob controle, também gostaríamos que $p = p_n$, que converge a zero.
Poderíamos por exemplo escolher
\begin{equation}
  p_n = \frac \lambda n.
\end{equation}
Mas a discussão que se segue é muito mais geral que essa escolha específica.

Como estaremos interessados em um regime assimtótico da distribuição de $X_p$ (lembre que apesar do espaço amostral de $X_n$ variar com $n$, sua distribuição é sempre uma probabilidade em $\mathbb{N}$).
Mas para falar de regimes assimtóticos, precisamos de definir uma noção de distância entre duas distribuições em $\mathbb{N}$.

\begin{definition}
Dadas duas distribuições $\mu_1$ e $\mu_2$ em $(\Omega, \mathcal{A})$, definimos
\begin{equation}
  \lVert \mu_1 - \mu_2 \rVert_{\VT} = \sup_{A \in \mathcal{A}} |\mu_1(A) - \mu_2(A)|,
\end{equation}
\index{mu1 - mu2@$\lVert \mu_1 - \mu_2 \rVert$} chamada de distância em variação total \index{variacao total@variação total} entre $\mu_1$ e $\mu_2$.
\end{definition}

No nosso caso, $\Omega$ é enumerável.
Vamos ver que nesse caso é possível reescrever a definição acima de modo a ver mais facilmente que se trata de uma distância no espaço de probabilidades em $\Omega$.

\begin{lemma}
  \label{l:vt_l1}
  Se $\Omega$ for finito ou enumerável, então podemos escrever
  \begin{equation}
    \lVert \mu_1 - \mu_2 \rVert_{\VT} = \frac{1}{2} \sum_{x \in \Omega} |\mu_1(x) - \mu_2(x)|.
  \end{equation}
\end{lemma}

\begin{proof}
Para mostrar que o lado esquerdo é maior ou igual ao direito, escolhemos $A = \{ x \in \Omega \, : \, \mu_2(x) \leq \mu_1(x)\}$. Assim
\begin{equation}
  \begin{split}
    \sum_{x \in A} \mu_1(x) - \mu_2(x) & = |\mu_1(A) - \mu_2(A)|\\
    & = |\mu_1(A^c) - \mu_2(A^c)| = \sum_{x \in A^c} \mu_2(x) - \mu_1(x),
  \end{split}
\end{equation}
donde
\begin{equation}
  \lVert \mu_1 - \mu_2 \rVert_{\VT} \geq |\mu_1(A) - \mu_2(A)| = \frac{1}{2} \sum_{i} |\mu_1(x_i) - \mu_2(x_i)|.
\end{equation}

Na outra direção, observe que para todo $B \subseteq \Omega$,
\begin{equation}
  \begin{split}
    \sum_{i} |\mu_1(x_i) - \mu_2(x_i)| & \geq \sum_{x \in B} \mu_1(x) - \mu_2(x) + \sum_{x \in B^c} \mu_1(x) - \mu_2(x)\\
    & = \mu_1(B) - \mu_2(B) + (1 - \mu_2(B)) - (1 - \mu_1(B))\\
    & = 2(\mu_1(B) - \mu_2(B)).
  \end{split}
\end{equation}
O que termina a prova do lema.
\end{proof}

Fica agora claro que $\lVert \mu_1 - \mu_2 \rVert_{\VT}$ determina uma distância.

\begin{exercise}
Mostre um lema análogo ao anterior para $(\Omega, \mathcal{A})$ qualquer, desde que $\mu_1$ e $\mu_2$ sejam absolutamente contínuas com relação à uma medida fixa nesse espaço mensurável. Nesse caso utilizaremos as derivadas de Radon–Nikodym.
\end{exercise}

Como estaremos interessados em variáveis independentes, precisamos de um resultado que relacione a distância em variação total com produtos de medida. Isso é parte do seguinte

\begin{lemma}
\label{l:vt_produto}
Sejam $\mu_1, \mu_2$ distribuições em $\Omega$ e $\nu_1, \nu_2$ distribuições em $y$ ambos enumeráveis. Então
\begin{equation}
  \lVert \mu_1 \otimes \nu_1 - \mu_2 \otimes \nu_2 \rVert_{\VT} \leq \lVert \mu_1 - \mu_2 \rVert_{\VT} + \lVert \nu_1 - \nu_2 \rVert_{\VT}.
\end{equation}
\end{lemma}

\begin{proof}
Basta expandir
\begin{equation}
  \begin{split}
    2\lVert \mu_1 & \otimes \nu_1 - \mu_2 \otimes \nu_2 \rVert_{\VT} = \sum_{x \in \Omega, y \in \Omega} |\mu_1(x)\nu_1(y) - \mu_2(x)\nu_2(y)|\\
    & \leq \sum_{x \in \Omega, y \in \Omega} |\mu_1(x)\nu_1(y) - \mu_1(x)\nu_2(y)| + |\mu_1(x)\nu_2(y) - \mu_2(x)\nu_2(y)|\\
    & \leq 2\lVert \mu_1 - \mu_2 \rVert_{\VT} + 2\lVert \nu_1 - \nu_2 \rVert_{\VT}.
  \end{split}
\end{equation}
Onde acima nós usamos que $\mu_1$ e $\nu_2$ são probabilidades. Isso termina a prova do lema.
\end{proof}

Finalmente, gostaríamos de entender como a distância de variação total se comporta com respeito à soma de variáveis independentes.
Isso estará ligado à convolução de distribuições:

\begin{definition}
Dadas, $\mu$ e $\nu$ distribuições em $\mathbb{Z}$, definimos a distribuição
\begin{equation}
  (\mu \star \nu)(x) := \sum_{y \in \mathbb{Z}} \mu(x-y) \nu(y).
\end{equation}
\end{definition}

Essa definição se relaciona com a soma de variáveis independentes graças ao seguinte
\begin{exercise}
Se $X \overset{d}\sim \mu$ e $Y \overset{d}\sim \nu$ são variáveis aleatórias inteiras e independentes, então $X + Y \overset{d}\sim \mu \star \nu$.
Dica: particione o espaço amostral nos eventos $[X = j]$, para $j \in \mathbb{Z}$, como na prova do Lema~\ref{l:soma_poisson} abaixo.
\end{exercise}

\begin{corollary}
Se $\mu$ e $\nu$ são distribuições em $\mathbb{Z}$, então $\mu \star \nu = \nu \star \mu$.
\end{corollary}

Como prometido, obtemos a seguinte relação entre a convolução e a distância de  variação total.
\begin{lemma}
\label{l:vt_conv}
Sejam $\mu$, $\nu$ duas medidas em $\Omega$ enumerável e $X:\  (\Omega,\mathcal{P}(\Omega))\to (E,\mathcal{A})$ um elemento aleatorio
\begin{equation}
   \lVert X_*\mu - X_*\nu\rVert_{\VT}\le    \lVert \mu - \nu\rVert_{\VT}.
\end{equation}
Em particular se $\mu_1, \mu_2, \nu_1, \nu_2$ são distribuições em $\mathbb{Z}$, então
\begin{equation}
  \lVert \mu_1 \star \nu_1 - \mu_2 \star \nu_2 \rVert_{\VT} \leq \lVert \mu_1 \otimes \nu_1 - \mu_2 \otimes \nu_2 \rVert_{\VT}
\end{equation}
\end{lemma}

\begin{proof}
O segundo ponto segue do primeiro applicado ao caso $\Omega = \mathbb{Z}^2$, $E=\mathbb{Z}$ e $X:\ (x,y) \mapsto (x+y)$.
Pelo primeiro, observamos
\begin{equation}
  \begin{split}
    2\lVert X_*\mu - X_*\nu\rVert_{\VT} &= \sum_{x \in E} \Big| \mu(X(\omega)=x) - \nu(X(\omega)=x) \Big|\\
    & =  \sum_{x \in E} \big| \sum_{\{\omega \in\Omega \ : \ X(\omega)=x\}}  \mu(\omega) - \nu(\omega) \big|\\
    & \leq \sum_{\omega \in \Omega} \big| \mu(\omega)- \nu(\omega) \big|\\
    & = 2\lVert \mu - \nu\rVert_{\VT}.
  \end{split}
\end{equation}
provando o lema.
\end{proof}

Para enunciar o resultado principal dessa seção, vamos apresentar uma distribuição em $\mathbb{N}$ bastane importante, que em particular se comporta muito bem com respeito a somas de variáveis independentes, como veremos.

\begin{definition}
  Uma variável aleatória $X$ é dita ter distribuição de Poisson \index{distribuicao@distribuição!de Poisson} com parâmetro $\lambda$, se
  \begin{equation}
    P[X = k] = \frac{\lambda^k e^{-\lambda}}{k!}, \text{ para $k \geq 0$ inteiro.}
  \end{equation}
  Denotamos isso por $X \overset{d}\sim \Poisson(\lambda)$.
\end{definition}

A distribuição de Poisson se comporta bem com respeito a somas independentes, como mostra o seguinte
\begin{lemma}
\label{l:soma_poisson}
Sejam $X \overset{d}\sim \Poisson(\lambda_1)$ e $Y \overset{d}\sim \Poisson(\lambda_2)$ independentes, então $X+Y \overset{d}\sim \Poisson(\lambda_1 + \lambda_2)$.
\end{lemma}

\begin{proof}
Basta calcular
\begin{equation}
  \begin{split}
    P[X+Y = k] & = \sum_{j = 0}^k P[X = j, Y = k-j] = \sum_{j = 0}^k \frac{\lambda_1^j e^{-\lambda_1} \lambda_2^{k-j} e^{-\lambda_2}}{j! (k-j)!}\\
    & = e^{-(\lambda_1 + \lambda_2)} \frac{1}{k!} \sum_{j = 0}^k \frac{k!}{j! (k-j)!} \lambda_1^j \lambda_2^{k-j} = \frac{e^{(\lambda_1 + \lambda_2)} (\lambda_1 + \lambda_2)^k}{k!},
  \end{split}
\end{equation}
mostrando o resultado.
\end{proof}

Nossa próxima tarefa é estimar a distância entre uma variável aleatória com distribuição $\Ber(p)$ e uma $\Poisson(p)$, como segue.

\begin{lemma}
\index{Lei!dos Pequenos Numeros@dos Pequenos Números}
\label{l:vt_ber_poiss}
Para $p \in [0,1]$, seja $\mu_1 = \Ber(p)$ e $\mu_2 = \Poisson(p)$, então,
\begin{equation}
  \lVert \mu_1 - \mu_2 \rVert_{\VT} \leq p^2.
\end{equation}
\end{lemma}

\begin{proof}
Sabemos que
\begin{equation}
  \begin{split}
    \lVert \mu_1 - \mu_2 \rVert_{\VT} & = \frac{1}{2} \sum_{x} |\mu_1(x) - \mu_2(x)|\\
    & = \frac{1}{2} \Big( |\mu_1(0) - \mu_2(0)| + |\mu_1(1) - \mu_2(1)| + \sum_{x \geq 2} \mu_2(x) \Big)\\
    & = \frac{1}{2} \Big( e^{-p} - (1-p) + p(1-e^{-p}) + (1 - e^{-p} - p e^{-p}) \Big)\\
    & = \frac{2}{2} p (1 - e^{-p}) \leq p^2,
  \end{split}
\end{equation}
terminando a prova.
\end{proof}

O teorema principal de convergência dessa seção concerne a soma de variáveis Bernoulli.

\begin{theorem}[Lei dos Pequenos Números]
  \label{t:lei_peq_numeros}
  Dado, $n \geq 1$ e $p \in [0,1]$, suponha que $\Omega_n$, $\mathcal{F}_n$ e $P_p$ sejam dados como em \eqref{e:Poisson_setup}.
  Então,
  \begin{equation}
    \lVert \Bin(n,p) - \Poisson(pn) \rVert_{\VT} \leq n p^2.
  \end{equation}
\end{theorem}

\begin{proof}
  Basta observar que
  \begin{equation}
    \begin{split}
      \lVert X_n \circ P_p - \Poisson(pn) \rVert_{\VT} & \overset{\text{Lema~\ref{l:soma_poisson}}}= \lVert \Ber(p)^{\star n} - \Poisson(p)^{\star n} \rVert_{\VT}\\
      \overset{\text{Lema~\ref{l:vt_conv}}}\leq & \lVert \Ber(p)^{\otimes n} - \Poisson(p)^{\otimes n} \rVert_{\VT}\\
      \overset{\text{Lema~\ref{l:vt_produto}}}\leq & n \lVert \Ber(p) - \Poisson(p) \rVert_{\VT} \overset{\text{Lema~\ref{l:vt_ber_poiss}}}\leq n p^2,
    \end{split}
  \end{equation}
  provando o teorema.
\end{proof}

\begin{corollary}
  No mesmo contexto do teorema acima, se $p = \lambda/n$, então temos
  \begin{equation}
    \lVert \Bin(n,p) - \Poisson(pn) \rVert_{\VT} \leq \lambda^2 / n,
  \end{equation}
  que converge a zero com $n$.
\end{corollary}
Veremos mais tarde que existem outros tipos de convergência.


\begin{exercise}
  Fixado $\lambda > 0$, seja $N$ uma variável aleatória com distribuição Poisson($\lambda$), isto é
  \begin{equation}
    P[N = k] = \frac{\lambda^k e^{-\lambda}}{k!} \text{ para $k = 0, 1, \dots$}
  \end{equation}
  Considere no mesmo espaço de probabilidade uma sequência de variáveis aleatórias $X_1, X_2, \dots$ que sejam \iid, com distribuição $\Ber(1/2)$ e independentes de $N$.
  \begin{enumerate}[\quad a)]
  \item Calcule a distribuição de $Z = \sum_{i=1}^N X_i$.
  \item Mostre que $Z$ e $N - Z$ são independentes.
  \end{enumerate}
\end{exercise}

\end{topics}


\begin{topics}

\section{Tópico: Percolação}
\label{s:percolacao}

Imagine que gostaríamos de modelar o movimento de um líquido em um meio poroso, como uma rocha ou uma esponja.
A primeira tarefa nesse estudo seria modelar esse meio poroso de maneira matematicamente rigorosa, que é o que faremos a seguir.

Fixamos uma dimensão $d \geq 1$ e consideramos o seguinte grafo $(\mathbb{Z}^d, E)$,
onde a rede quadrada $\mathbb{Z}^d$ é o conjunto de vértices e o conjunto de elos é dado por
\begin{equation*}
  E = \big\{ \{x, y\} \subset \mathbb{Z}^d \, : \,  |x - y| = 1 \},
\end{equation*}
onde $|\cdot|$ representa a distância euclideana em $\mathbb{R}^d$.

No nosso modelo, esse grafo pode ser entendido como um cristal periódico onde cada vértice representa uma cavidade do material poroso e os elos são potenciais conexões entre poros vizinhos.

Até agora nosso grafo é apenas uma rede periódica, mas as coisas começam a ficar interessantes à partir de agora.
Imaginamos que nosso material poroso está sujeito a variações durante sua formação.
Isso se reflete no fato que alguns elos de $E$ podem estar abertos ou não aleatoriamente.

Para o nosso modelos, o espaço amostral vai ser $\Omega:= \{0,1\}^E$ considerado com a $\sigma$-algebra produto.
Fixamos um $p \in [0,1]$ e definimos uma coleção de variáveis aleatórias $\omega_e$, para $e \in E$, que sejam \iid e com distribuição $\Ber(p)$.
Chamamos $P_p$ a probabilidade corespondente.
Essas variáveis aleatórias induzem um grafo aleatorio $G(\omega)=(\mathbb{Z}^d, \mathcal{E}(\omega))$, subgrafo do grafo original,
que corresponde a incluir apenas os elos $e$ com $\omega_e = 1$.
Mais precisamente
\begin{equation}
  \mathcal{E}(\omega) = \big\{ e \in E\, : \, \omega_e = 1 \big\}.
\end{equation}
Podemos ver na Figura~\ref{f:percola} algumas simulações desse grafo aleatório.

\begin{figure}[!ht]
  \centering
  \begin{tikzpicture}[scale=.1]
    \ifdraft{\def\side{5}}{\def\side{30}}
    \draw[step=1, color=gray!50!white] (1,1) grid (\side + 1, \side + 1)
                                       (41, 1) grid (41 + \side, \side + 1) (81, 1) grid (81 + \side, \side + 1);
    \foreach \x in {1,...,\side}
    { \foreach \y in {1,...,\side}
      { \pgfmathrandominteger{\a}{0}{100} \ifthenelse{\a>60}{\draw[thick] (\x, \y) -- (\x, \y + 1);}{}
        \pgfmathrandominteger{\a}{0}{100} \ifthenelse{\a>60}{\draw[thick] (\x, \y) -- (\x + 1, \y);}{} } }
    \foreach \x in {1,...,\side}
    { \foreach \y in {1,...,\side}
      { \pgfmathrandominteger{\a}{0}{100} \ifthenelse{\a>50}{\draw[thick] (40 + \x, \y) -- (40 + \x, \y + 1);}{}
        \pgfmathrandominteger{\a}{0}{100} \ifthenelse{\a>50}{\draw[thick] (40 + \x, \y) -- (40 + \x + 1, \y);}{} } }
    \foreach \x in {1,...,\side}
    { \foreach \y in {1,...,\side}
      { \pgfmathrandominteger{\a}{0}{100} \ifthenelse{\a>40}{\draw[thick] (80 + \x, \y) -- (80 + \x, \y + 1);}{}
        \pgfmathrandominteger{\a}{0}{100} \ifthenelse{\a>40}{\draw[thick] (80 + \x, \y) -- (80 + \x + 1, \y);}{} } }
  \end{tikzpicture}
  \caption{Três simulações do grafo aleatório $(\mathbb{Z}^d, \mathcal{E})$, para valores de $p = 0,4$ (esquerda), $p = 0,5$ (centro) e $p = 0,6$ (direita). Tente imaginar como seria caminhar nesse grafo como se ele fosse um labirinto.}
  \label{f:percola}
\end{figure}

Agora que temos um modelo de meio poroso bem definido, precisamos pensar em quais perguntas nos interessam sobre $\mathcal{G} = (\mathbb{Z}^d, \mathcal{E})$.
Sendo esse um modelo poara passagem de fluido, as primeiras perguntas que faremos concerne a conectividade de $\mathcal{G}$.

\begin{exercise}
  Mostre que quase certamente $G(\omega)$ é desconexo.
  Mais precisamente, mostre que existem quase certamente infinitos vértices isolados em $G(\omega)$.
\end{exercise}

Como não podemos esperar que $G(\omega)$ seja conexo, podemos nos perguntar algo mais fraco, como por exemplo se a
componente conexa da origem $0 \in \mathbb{Z}^d$ em $G(\omega)$ é infinita.

Voltando à Figura~\ref{f:percola} vemos que, dependendo do valor de $p \in [0,1]$, pode ser bem difícil ou bem fácil encontrar um caminho longo à partir da origem.
Isso é uo que estudaremos em mais detalhes no que segue.

Mais precisamente estamos interessados em:
\begin{equation}
  A = \big\{\omega \in \Omega \, : \,  \text{ a componente conexa de $0 \in \mathbb{Z}^d$ em $G(\omega)$ é infinita} \big\}.
\end{equation}

Para estudar $A$, vamos fazer uma aproximação de $A$ por eventos mais simples
\begin{equation}
  A_n = \big\{ \omega \in \Omega \, : \, \text{ a componente conexa de $0$ sai da caixa $[-n, n]^d$}\},
\end{equation}
para $n \geq 1$.

\begin{exercise}
  Mostre que $A = \cap_{n=1}^n A_n$ e consequentemente que $A$ é de fato mensurável e $P(A) = \lim_{n\to \infty} P(A_n)$.
\end{exercise}

Definimos portanto a função $\theta:[0,1] \to [0,1]$ por
\begin{equation}
  \theta(p) = P_p(A),
\end{equation}
onde $P_p$ denota a probabilidade correspondente ao valor escolhido de $p \in [0,1]$.

\begin{exercise}
  Mostre que $\theta(p) \leq 1-(1-p)^{2d}$.
\end{exercise}

Nosso objetivo é entender algumas das propriedades de $\theta$.
A nossa intuição diz que quanto maior o valor de $p$, mais elos serão abertos em $\mathcal{G}$ e portanto maior será o valor de $\theta$, ou em outras palavras, $\theta$ deve ser monótona não decrescente.

\begin{exercise}
  Construiremos nosso modelo de uma maneira alternativa num espaço de probabilidade maior.
  Definimos $\Omega_0:=[0,1]^E$ (com a $\sigma$-álgebra produto correspondente), e $(U_e)_{e\in E}$
  uma coleção de variáveis aleatórias \iid com distribuição $U[0,1]$, e  $\mathbb{P}$ a probabilidade corespondente.
  Definimos para cada $p \in [0,1]$, $X^p : \Omega_0 \to \Omega$ do jeito seguinte
  \begin{equation}
    X^p_e = \1_{[\omega_e \leq p]}.
  \end{equation}
  Mostre que para todo $p \in [0,1]$  $(X^p)_*\mathbb{P}=P_p$.
  Use isso para concluir que $\theta$ é monótona não decrescente.
\end{exercise}

Iremos agora mostrar a existência de um regime para o qual a componente conexa da origem não é infinita.

\begin{theorem}
  Para $p < 1/(2d)$, temos que $\theta(p) = 0$.
\end{theorem}

Antes da prova, alguns exercícios.

\begin{exercise}
  Definimos um caminho como sendo uma sequência $x_1$, $\dots$, $x_k$ ($k \in \mathbb{N}$), tal que $\{x_i, x_{i+1}\} \in E$ para todo $i = 1, \dots, k-1$.
  Tal caminho é dito aberto se $\omega_{\{x_i, x_{i+1}\}} = 1$ para todo $i \leq k-1$.
  E dizemos que ele é auto-evitante se $x_i \neq x_j$ para todo $1 \leq i < j < k$.
  Mostre que
  \begin{equation*}
    \begin{split}
      & A_n = \Big\{ \omega \in \Omega \, : \,  \text{ existe um caminho aberto $(x_i)_{i=1}^{k}$ com $x_1 = 0$ e $x_k \not \in [-n, n]^d$} \Big\}\\
      & A_n = \big\{ \omega \in \Omega \, : \, \text{ existe um caminho auto-evitante como acima} \big\}.
    \end{split}
  \end{equation*}
\end{exercise}

\begin{proof}
  Dado $p < 1/(2d)$ e $n \in \mathbb{N}$, lembramos que
  \begin{equation*}
    \begin{split}
      \theta(p) & \leq P_p(A_n) = P_p \Big[
      \begin{array}{c}
      \text{existe $k \in \mathbb{N}$ e um caminho auto-evitante $(x_i)_{i=1}^k$ }\\
      \text{aberto e com $x_1 = 0$ e $x_k \not \in [-n, n]^d$}
    \end{array} \Big]\\[2mm]
    & \leq \sum_{k \geq n} \; \; \sum_{(x_i)_{i=1}^k \text{ auto-evit.}} P_p [(x_i)_{i=1}^k \text{ aberto}] = \sum_{k \geq n} \; \; \sum_{(x_i)_{i=1}^k \text{ auto-evit.}} p^k\\
    & \leq \sum_{k \geq n} \; \; \sum_{(x_i)_{i=1}^k \text{ caminho}} P_p [(x_i)_{i=1}^k \text{ aberto}] = \sum_{k \geq n} (2d)^k p^k.
    \end{split}
  \end{equation*}
  Como $p < 1/(2d)$, a soma acima é finita e converge a zero quando $n$ diverge, provando o teorema.
\end{proof}

{\bf Notas} - O teorema acima ajuda a compreender o comportamento que observamos no lado esquerdo da Figura~\ref{f:percola}.
Mais precisamente, ele nos diz que para valores de $p$ baixos (na verdade $0,4$ não é baixo o suficiente para podermos aplicar esse teorema) é difícil encontrar um caminho aberto do centro à borda da caixa.

Na verdade, é possível mostrar que para $d = 2$,
\begin{equation}
  \begin{split}
    & \text{$\theta(p) = 0$ para todo $p \leq 1/2$ e}\\
    & \text{$\theta(p) > 0$ para todo $p > 1/2$,}
  \end{split}
\end{equation}
como foi mostrado por Harris e Kesten, veja por exemplo \cite{Gri99} e \cite{bollobas2006percolation}.
De fato, algo bastante interessante está acontecendo nesse modelo para $p = 1/2$, como nos mostrou o trabalho de grandes matemáticos, como: Oded Schramm, Wendelin Werner, Stanislav Smirnov, entre outros.

\todosec{Tópico: Teorema de Uma Série}{fazer...}

\end{topics}


\end{document}
